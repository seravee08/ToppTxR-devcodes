{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run DMT.ipynb\n",
    "%run FileIO.ipynb\n",
    "%run viewer.ipynb\n",
    "from scipy import ndimage\n",
    "\n",
    "#FileIO_DMT.cvt_img2dipha_('E:/Data2/Test/retina_test04.png', 'E:/Data2/Test/complex.bin')\n",
    "#img1 = FileIO_DMT.cvt_dipha2img_('E:/Data2/Test/complex.bin')\n",
    "\n",
    "# ===== Read input binary file\n",
    "# result = FileIO.read_matrix_binary('E:/Data2/Cremi_res/128_1_1_0/gen_00000.dat', 'f')\n",
    "# viewer.imshow_(result, 'jet')\n",
    "\n",
    "# ===== Read saved sampled results\n",
    "result = FileIO.read_matrix_binary('E:/Data2/Cremi_res/128_1_1_0/gen_00127.dat', 'f')\n",
    "result_gx = ndimage.sobel(result,axis=0,mode='constant')\n",
    "result_gy = ndimage.sobel(result,axis=1,mode='constant')\n",
    "result_gr = np.hypot(result_gx, result_gy)\n",
    "\n",
    "thresh = 0.1\n",
    "parti, label_num = DMT.PH_watershed(result_gr, thresh, verbose = False, test = True)\n",
    "parti = Util_gen.shuffle_partition_label(parti, 1, \"nosave\")[0]\n",
    "print(label_num)\n",
    "viewer.imshow_(result_gr)\n",
    "viewer.imshow_(result, 'jet')\n",
    "viewer.imshow_(parti, 'jet')\n",
    "\n",
    "\n",
    "\n",
    "# result_gr[result_gr > 0.2] = 8\n",
    "# viewer.imshow_(result_gr, 'gray')\n",
    "# viewer.imshow_(result, 'jet')\n",
    "\n",
    "# img = FileIO.read_matrix_binary('E:/Data2/Cremi/wobound/nom/cremi_00000_0.dat', 'i')\n",
    "# img_gx = ndimage.sobel(img,axis=0,mode='constant')\n",
    "# img_gy = ndimage.sobel(img,axis=1,mode='constant')\n",
    "# img_gr = np.hypot(img_gx, img_gy)\n",
    "# viewer.imshow_(img_gr, 'gray')\n",
    "# viewer.imshow_(img, 'jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run DMT.ipynb\n",
    "%run FileIO.ipynb\n",
    "%run viewer.ipynb\n",
    "%run Utility_general.ipynb\n",
    "\n",
    "# FileIO.img_connected_component_cv_batch_('E:/Data2/Cremi/wobound/seg', 'E:/Data2/Cremi/wobound/nom', 'png', binarize=True, fillRift=True, shuffle_num=100)\n",
    "# FileIO.img_boundbox_batch_('E:/Data2/Test', 'E:/Data2/Test/bbx', 'png', 2)\n",
    "# FileIO_DMT.cvt_img2dipha_batch_('E:/Data2/Test', 'E:/Data2/Test', 'png', ['dist_trfm', True, True])\n",
    "\n",
    "path = 'E:/Data2/Retina/wobound/seg/retina_0084.png'\n",
    "# dt = FileIO_DMT.cvt_dipha2img_(path)\n",
    "img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "viewer.imshow_(img)\n",
    "\n",
    "dt = Util_gen.dist_trfm(img, binarize=True, inverse=True)\n",
    "viewer.imshow_(dt)\n",
    "\n",
    "thresh = 0.1\n",
    "parti, label_num = DMT.PH_watershed(dt, thresh, verbose = False, test = True)\n",
    "print(label_num)\n",
    "viewer.imshow_(parti, 'jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DMT.ipynb\n",
    "%run FileIO.ipynb\n",
    "%run viewer.ipynb\n",
    "%run Utility_general.ipynb\n",
    "\n",
    "path = 'E:/Data2/Cremi/wobound/seg/cremi_00000.png'\n",
    "img  = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "img[img>127]  = 255\n",
    "img[img<=127] = 0\n",
    "viewer.imshow_(img)\n",
    "cnt, hry, red = Util_cv.compute_bnd_red_cv(img, 0, 255, 8)\n",
    "viewer.draw_red_on_single_dim_cv(red, component_to_plot=-1)\n",
    "m = Util_gen.fill_rift_(red[1], 8)\n",
    "viewer.imshow_(m, 'jet')\n",
    "print(np.amax(m), np.amin(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run Archpool.ipynb\n",
    "%run Argparser.ipynb\n",
    "%run GAN.ipynb\n",
    "\n",
    "settings    = return_settings()\n",
    "parser      = Argparser(settings)\n",
    "general     = return_general(0)\n",
    "G_arch      = return_arch(6)\n",
    "D_arch      = return_arch(7)\n",
    "data_params = return_data_settings(0)\n",
    "adv_params  = return_advanced_params()\n",
    "\n",
    "def run():\n",
    "    gan = GAN(general, adv_params, G_arch, D_arch)\n",
    "    #gan.train(data_params)\n",
    "    gan.sample_save_('128_128_1_1_0.dat', [128, 128, 1, 1], 'E:/Data2/Cremi_res/128_1_1_0', 0, True)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run Archpool.ipynb\n",
    "%run Argparser.ipynb\n",
    "%run FileIO.ipynb\n",
    "%run CNN.ipynb\n",
    "\n",
    "%run Cyclekernel.ipynb\n",
    "%run Medical_IO.ipynb\n",
    "%run Medical_Utility.ipynb\n",
    "\n",
    "%run GradCam_Utility.ipynb\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "settings    = return_settings()\n",
    "parser      = Argparser(settings)\n",
    "general     = return_general(0)\n",
    "arch1       = return_arch(16)\n",
    "#arch2       = return_arch(142)\n",
    "data_params = return_data_settings(0)\n",
    "\n",
    "cnn = CNN(general, [arch1])\n",
    "#cnn.train(data_params)\n",
    "cnn.test(data_params)\n",
    "#heatmap, vol = cnn.test(data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_id = 1001\n",
    "# seg = FileIO_MEDICAL.load_nii(\"E:/Data2/BreastMass_refine/segmentations_srk/ISPY_\"+str(target_id)+\"_seg_srk.nii\")\n",
    "# tum = FileIO_MEDICAL.load_nii(\"E:/Data2/BreastMass_refine/tumors_srk/ISPY_\"+str(target_id)+\"_tum_srk.nii\")\n",
    "# vol = FileIO_MEDICAL.read_dat(\"E:/Data2/BreastMass_refine/sup/ISPY_\"+str(target_id)+\"_vol2_sup.dat\")\n",
    "\n",
    "# vol = vol + np.abs(np.amin(vol)) + 1\n",
    "# pers = FileIO_MEDICAL.read_pers_txt(\"E:/Data2/BreastMass_refine/sup/ISPY_\"+str(target_id)+\"_vol2_sup.dat.pers.txt\")\n",
    "# bnd = FileIO_MEDICAL.read_bnd_red_unifieddim(\"E:/Data2/BreastMass_refine/sup/ISPY_\"+str(target_id)+\"_vol2_sup.dat.bnd\")\n",
    "# bnd_filt = Compute_CycleKernel.filter_bnd_or_red(bnd, pers, 200)\n",
    "\n",
    "# bnd_drw = Utility_MEDICAL.draw_on_volume_ori_intensity(bnd_filt, vol, seg, tum, 10, 0, 1, np.arange(0,len(bnd_filt[1])), [500,1500])\n",
    "# bnd_drw2 = Utility_MEDICAL.draw_on_volume_ori_intensity(bnd_filt, vol, seg, tum, 10, 0, 2, np.arange(0,len(bnd_filt[2])), [800,1200])\n",
    "\n",
    "# viewer_3D.tmp5_viewer(vol, bnd_drw, bnd_drw2, heatmap)\n",
    "# viewer_3D.tmp2_viewer(vol, heatmap)\n",
    "\n",
    "import cv2\n",
    "\n",
    "vol_slice = vol[100,:]\n",
    "heatmap_slice = heatmap[100,:]\n",
    "\n",
    "print(np.amax(vol_slice))\n",
    "print(np.amin(vol_slice))\n",
    "print(np.amax(heatmap_slice))\n",
    "print(np.amin(heatmap_slice))\n",
    "\n",
    "plt.matshow(vol_slice, cmap='gray')\n",
    "plt.matshow(heatmap_slice, cmap='gray')\n",
    "\n",
    "heatmap_slice = np.amin(heatmap_slice) + heatmap_slice\n",
    "#heatmap_slice = heatmap_slice# * 150\n",
    "\n",
    "#heatmap_slice = np.uint8(255*heatmap_slice)\n",
    "#heatmap_slice = cv2.applyColorMap(heatmap_slice, cv2.COLORMAP_JET)\n",
    "superimposed_img = heatmap_slice + vol_slice\n",
    "plt.matshow(superimposed_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run FileIO.ipynb\n",
    "%run Argparser.ipynb\n",
    "%run Archpool.ipynb\n",
    "%run CNN_3Dmnist.ipynb\n",
    "\n",
    "settings    = return_settings()\n",
    "parser      = Argparser(settings)\n",
    "arch = return_arch(13)\n",
    "path = 'E:/Data2/Mnist3D/full_dataset_vectors.h5'\n",
    "X_train, X_test, targets_train, targets_test = FileIO.read_h5py(path, (16,16,16,3))\n",
    "\n",
    "cnn_mnist = CNN_3Dmnist(arch)\n",
    "cnn_mnist.train(X_train, X_test, targets_train, targets_test)\n",
    "\n",
    "# sal = cnn.test(X_test, targets_test)\n",
    "# viewer_3D.tmp_viewer(sal[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Medical_Utility.ipynb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "xx = np.array((),dtype=np.int32)\n",
    "x0_ = np.array([0, 0, 1, 1])\n",
    "y0_ = np.array([1, 1, 0, 1])\n",
    "x1_ = np.array([0, 0, 1, 0])\n",
    "y1_ = np.array([0, 0, 1, 1])\n",
    "c = np.concatenate((xx, x0_,x1_))\n",
    "print(c)\n",
    "\n",
    "# sx = roc_auc_score(y0_, x0_)\n",
    "# sy = roc_auc_score(y1_, x1_)\n",
    "# print((sx+sy)/2)\n",
    "\n",
    "# print(roc_auc_score(y0_, x0_))\n",
    "# print(Utility_MEDICAL.binary_auc_score(y0_, x0_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ts_02_dil2_acc = [0.6667, 0.7500, 0.6875, 0.7500, 0.7500]\n",
    "ts_02_dil2_bacc = [0.6890, 0.7462, 0.6791, 0.7220, 0.7220]\n",
    "ts_02_dil2_spec = [0.6154, 0.6154, 0.5385, 0.5385, 0.5385]\n",
    "ts_02_dil2_sens = [0.6857, 0.8000, 0.7429, 0.8286, 0.8286]\n",
    "ts_02_dil2_auc = [0.6505, 0.7077, 0.6407, 0.6835, 0.6835]\n",
    "\n",
    "dim2_02_dil2_acc = [0.7292, 0.6458, 0.7083, 0.6458, 0.7292]\n",
    "dim2_02_dil2_bacc = [0.7135, 0.6264, 0.6692, 0.6264, 0.7135]\n",
    "dim2_02_dil2_spec = [0.5294, 0.4615, 0.4615, 0.44615, 0.5294]\n",
    "dim2_02_dil2_sens = [0.8387, 0.7143, 0.8000, 0.7143, 0.8387]\n",
    "dim2_02_dil2_auc = [0.6841, 0.5879, 0.6308, 0.5879, 0.6841]\n",
    "\n",
    "mri_raw_acc = [0.5833, 0.5208, 0.5833, 0.5000, 0.5417]\n",
    "mri_raw_bacc = [0.5474, 0.4858, 0.5342, 0.4696, 0.5417]\n",
    "mri_raw_spec = [0.2941, 0.2353, 0.2353, 0.2353, 0.4118]\n",
    "mri_raw_sens = [0.7419, 0.6774, 0.7742, 0.6452, 0.6129]\n",
    "mri_raw_auc = [0.5180, 0.4564, 0.5047, 0.4402, 0.5123]\n",
    "\n",
    "ts_07_dil2_acc = [0.6667, 0.7292, 0.6042, 0.7083, 0.6667]\n",
    "ts_07_dil2_bacc = [0.6407, 0.7077, 0.5495, 0.6451, 0.6407]\n",
    "ts_07_dil2_spec = [0.4615, 0.5385, 0.3077, 0.3846, 0.4615]\n",
    "ts_07_dil2_sens = [0.7429, 0.8000, 0.7143, 0.8286, 0.7429]\n",
    "ts_07_dil2_auc = [0.6022, 0.6692, 0.5110, 0.6066, 0.6022]\n",
    "\n",
    "ts_2_dil2_acc = [0.6458, 0.6042, 0.6458, 0.6458, 0.6042]\n",
    "ts_2_dil2_bacc = [0.5538, 0.5978, 0.6505, 0.6505, 0.5495]\n",
    "ts_2_dil2_spec = [0.2308, 0.4615, 0.5385, 0.5385, 0.3077]\n",
    "ts_2_dil2_sens = [0.8000, 0.6571, 0.6857, 0.6857, 0.7143]\n",
    "ts_2_dil2_auc = [0.5154, 0.5593, 0.6121, 0.6121, 0.5110]\n",
    "\n",
    "dim1_02_dil2_acc = [0.6458, 0.6875, 0.6667, 0.7500, 0.6458]\n",
    "dim1_02_dil2_bacc = [0.6022, 0.6791, 0.6165, 0.6253, 0.6505]\n",
    "dim1_02_dil2_spec = [0.3846, 0.5385, 0.3846, 0.2308, 0.5385]\n",
    "dim1_02_dil2_sens = [0.7429, 0.7429, 0.7714, 0.9429, 0.6857]\n",
    "dim1_02_dil2_auc = [0.5637, 0.6407, 0.5780, 0.5868, 0.6121]\n",
    "\n",
    "ts_02_dil4_acc = [0.7083, 0.6667, 0.6875, 0.6458]\n",
    "ts_02_dil4_bacc = [0.6451, 0.6407, 0.6308, 0.6505]\n",
    "ts_02_dil4_spec = [0.3846, 0.4615, 0.3846, 0.5385]\n",
    "ts_02_dil4_sens = [0.8286, 0.7429, 0.8000, 0.6857]\n",
    "ts_02_dil4_auc = [0.6066, 0.6022, 0.5923, 0.6121]\n",
    "\n",
    "ts_02_dil8_acc = [0.6667, 0.6042, 0.6875, 0.6667, 0.6042]\n",
    "ts_02_dil8_bacc = [0.6407, 0.5495, 0.6549, 0.6165, 0.5736]\n",
    "ts_02_dil8_spec = [0.4615, 0.3077, 0.4615, 0.3846, 0.3846]\n",
    "ts_02_dil8_sens = [0.7429, 0.7143, 0.7714, 0.7714, 0.6857]\n",
    "ts_02_dil8_auc = [0.6022, 0.5110, 0.6165, 0.5780, 0.5352]\n",
    "\n",
    "# ================================================================\n",
    "baseline_rerun_acc  = [0.6878, 0.8438, 0.2500, 0.6875, 0.6970]\n",
    "baseline_rerun_bacc = [0.5500, 0.8171, 0.5625, 0.7367, 0.7212]\n",
    "baseline_rerun_spec = [0.0000, 0.5714, 1.0000, 0.6667, 0.6154]\n",
    "baseline_rerun_sens = [1.0000, 0.9200, 0.0000, 0.6957, 0.7500]\n",
    "baseline_rerun_auc  = [0.5000, 0.7457, 0.5000, 0.6812, 0.6827]\n",
    "baseline_rerun_f1   = [0.0000, 0.6154, 0.4000, 0.5455, 0.6154]\n",
    "\n",
    "baseline_rerun_acc_bat3  = [0.5938, 0.3125, 0.7812, 0.6250]\n",
    "baseline_rerun_bacc_bat3 = [0.6727, 0.5500, 0.7000, 0.6682]\n",
    "baseline_rerun_spec_bat3 = [0.7000, 1.0000, 0.3000, 0.6000]\n",
    "baseline_rerun_sens_bat3 = [0.5455, 0.0000, 1.0000, 0.6364]\n",
    "baseline_rerun_auc_bat3  = [0.6227, 0.5000, 0.6500, 0.6182]\n",
    "baseline_rerun_f1_bat3   = [0.5185, 0.4762, 0.4615, 0.5000]\n",
    "\n",
    "rr_dim12_02_dil0_acc  = [0.8438, 0.8438, 0.9062, 0.8438, 0.6970]\n",
    "rr_dim12_02_dil0_bacc = [0.8273, 0.9200, 0.9167, 0.8116, 0.7212]\n",
    "rr_dim12_02_dil0_spec = [0.6000, 0.8571, 0.7500, 0.5556, 0.6154]\n",
    "rr_dim12_02_dil0_sens = [0.9545, 0.8400, 0.9583, 0.9565, 0.7500]\n",
    "rr_dim12_02_dil0_auc  = [0.7773, 0.8486, 0.8542, 0.7560, 0.6827]\n",
    "rr_dim12_02_dil0_f1   = [0.7059, 0.7059, 0.8000, 0.6667, 0.6154]\n",
    "\n",
    "rr_dim12_07_dil0_acc  = [0.8438, 0.8750, 0.8750, 0.8438, 0.8182]\n",
    "rr_dim12_07_dil0_bacc = [0.8273, 0.8886, 0.8958, 0.8116, 0.8212]\n",
    "rr_dim12_07_dil0_spec = [0.6000, 0.7143, 0.7500, 0.5556, 0.6154]\n",
    "rr_dim12_07_dil0_sens = [0.9545, 0.9200, 0.9167, 0.9565, 0.9500]\n",
    "rr_dim12_07_dil0_auc  = [0.7773, 0.8171, 0.8333, 0.7560, 0.7827]\n",
    "rr_dim12_07_dil0_f1   = [0.7059, 0.7143, 0.7500, 0.6667, 0.7273]\n",
    "\n",
    "# rr_dim12_2_dil0_acc  = [0.8750, 0.9062, 0.8750, 0.7812, 0.8182]\n",
    "# rr_dim12_2_dil0_bacc = [0.8773, 0.9086, 0.9375, 0.8357, 0.8212]\n",
    "# rr_dim12_2_dil0_spec = [0.7000, 0.7143, 0.8750, 0.7778, 0.6154]\n",
    "# rr_dim12_2_dil0_sens = [0.9545, 0.9600, 0.8750, 0.7826, 0.9500]\n",
    "# rr_dim12_2_dil0_auc  = [0.8273, 0.8371, 0.8750, 0.7802, 0.7827]\n",
    "# rr_dim12_2_dil0_f1   = [0.7778, 0.7692, 0.7778, 0.6667, 0.7273]\n",
    "\n",
    "rr_dim1_2_dil0_acc   = [0.7812, 0.5938, 0.7188, 0.7188, 0.7812]\n",
    "rr_dim1_2_dil0_bacc  = [0.7545, 0.7273, 0.7091, 0.7636, 0.7818]\n",
    "rr_dim1_2_dil0_spec  = [0.5000, 0.9000, 0.5000, 0.7000, 0.6000]\n",
    "rr_dim1_2_dil0_sens  = [0.9091, 0.4545, 0.8182, 0.7273, 0.8636]\n",
    "rr_dim1_2_dil0_auc   = [0.7045, 0.6773, 0.6591, 0.7136, 0.7318]\n",
    "rr_dim1_2_dil0_f1    = [0.5882, 0.5806, 0.5263, 0.6087, 0.6316]\n",
    "\n",
    "rr_dim2_2_dil0_acc   = [0.7812, 0.8125, 0.7500, 0.7188, 0.7188]\n",
    "rr_dim2_2_dil0_bacc  = [0.7273, 0.7500, 0.7318, 0.7636, 0.7364]\n",
    "rr_dim2_2_dil0_spec  = [0.4000, 0.4000, 0.5000, 0.7000, 0.6000]\n",
    "rr_dim2_2_dil0_sens  = [0.9545, 1.0000, 0.8636, 0.7273, 0.7727]\n",
    "rr_dim2_2_dil0_auc   = [0.6773, 0.7000, 0.6818, 0.7136, 0.6864]\n",
    "rr_dim2_2_dil0_f1    = [0.5333, 0.5714, 0.5556, 0.6087, 0.5714]\n",
    "\n",
    "# =============================================================\n",
    "# =============================================================\n",
    "rr_dim12_100_dil0_acc  = [0.8125, 0.8438, 0.8750]\n",
    "rr_dim12_100_dil0_bacc = [0.8045, 0.8686, 0.8542]\n",
    "rr_dim12_100_dil0_spec = [0.6000, 0.7143, 0.6250]\n",
    "rr_dim12_100_dil0_sens = [0.9091, 0.8800, 0.9583]\n",
    "rr_dim12_100_dil0_auc  = [0.7545, 0.7971, 0.7917]\n",
    "rr_dim12_100_dil0_f1   = [0.6667, 0.6667, 0.7143]\n",
    "\n",
    "rr_dim12_55_dil0_acc  = [0.8438, 0.8750, 0.8125]\n",
    "rr_dim12_55_dil0_bacc = [0.8273, 0.8371, 0.9375]\n",
    "rr_dim12_55_dil0_spec = [0.6000, 0.5714, 1.0000]\n",
    "rr_dim12_55_dil0_sens = [0.9545, 0.9600, 0.7500]\n",
    "rr_dim12_55_dil0_auc  = [0.7773, 0.7657, 0.8750]\n",
    "rr_dim12_55_dil0_f1   = [0.7059, 0.6667, 0.7273]\n",
    "\n",
    "rr_dim12_35_dil0_acc  = [0.8438, 0.8438, 0.7812]\n",
    "rr_dim12_35_dil0_bacc = [0.8545, 0.8171, 0.8750]\n",
    "rr_dim12_35_dil0_spec = [0.7000, 0.5714, 0.8750]\n",
    "rr_dim12_35_dil0_sens = [0.9091, 0.9200, 0.7500]\n",
    "rr_dim12_35_dil0_auc  = [0.8045, 0.7457, 0.8125]\n",
    "rr_dim12_35_dil0_f1   = [0.7368, 0.6154, 0.6667]\n",
    "\n",
    "rr_dim12_17_dil0_acc  = [0.8125, 0.8750, 0.8750]\n",
    "rr_dim12_17_dil0_bacc = [0.7773, 0.8371, 0.8958]\n",
    "rr_dim12_17_dil0_spec = [0.5000, 0.5714, 0.7500]\n",
    "rr_dim12_17_dil0_sens = [0.9545, 0.9600, 0.9167]\n",
    "rr_dim12_17_dil0_auc  = [0.7273, 0.7657, 0.8333]\n",
    "rr_dim12_17_dil0_f1   = [0.6250, 0.6667, 0.7500]\n",
    "\n",
    "rr_dim12_06_dil0_acc  = [0.7812, 0.8750, 0.8125]\n",
    "rr_dim12_06_dil0_bacc = [0.8091, 0.8886, 0.8542]\n",
    "rr_dim12_06_dil0_spec = [0.7000, 0.7143, 0.7500]\n",
    "rr_dim12_06_dil0_sens = [0.8182, 0.9200, 0.8333]\n",
    "rr_dim12_06_dil0_auc  = [0.7591, 0.8171, 0.7917]\n",
    "rr_dim12_06_dil0_f1   = [0.6667, 0.7143, 0.6667]\n",
    "\n",
    "# =============================================================\n",
    "# Dilations\n",
    "# =============================================================\n",
    "rr_dim12_2_dil0_acc   = [0.8750, 0.9062, 0.8750]\n",
    "rr_dim12_2_dil0_bacc  = [0.8773, 0.9086, 0.9375]\n",
    "rr_dim12_2_dil0_spec  = [0.7000, 0.7143, 0.8750]\n",
    "rr_dim12_2_dil0_sens  = [0.9545, 0.9600, 0.8750]\n",
    "rr_dim12_2_dil0_auc   = [0.8273, 0.8371, 0.8750]\n",
    "rr_dim12_2_dil0_f1    = [0.7778, 0.7692, 0.7778]\n",
    "\n",
    "rr_dim12_2_dil1_acc   = [0.8750, 0.9062, 0.9062]\n",
    "rr_dim12_2_dil1_bacc  = [0.8500, 0.8571, 0.9167]\n",
    "rr_dim12_2_dil1_spec  = [0.6000, 0.5714, 0.7500]\n",
    "rr_dim12_2_dil1_sens  = [1.0000, 1.0000, 0.9583]\n",
    "rr_dim12_2_dil1_auc   = [0.8000, 0.7857, 0.8542]\n",
    "rr_dim12_2_dil1_f1    = [0.7500, 0.7273, 0.8000]\n",
    "\n",
    "rr_dim12_2_dil2_acc   = [0.8750, 0.8750, 0.8750]\n",
    "rr_dim12_2_dil2_bacc  = [0.8773, 0.9914, 0.9375]\n",
    "rr_dim12_2_dil2_spec  = [0.7000, 1.0000, 0.8750]\n",
    "rr_dim12_2_dil2_sens  = [0.9545, 0.8400, 0.8750]\n",
    "rr_dim12_2_dil2_auc   = [0.8273, 0.9200, 0.8750]\n",
    "rr_dim12_2_dil2_f1    = [0.7778, 0.7778, 0.7778]\n",
    "\n",
    "rr_dim12_2_dil3_acc   = [0.8438, 0.8438, 0.8750]\n",
    "rr_dim12_2_dil3_bacc  = [0.8273, 0.8686, 0.8958]\n",
    "rr_dim12_2_dil3_spec  = [0.6000, 0.7143, 0.7500]\n",
    "rr_dim12_2_dil3_sens  = [0.9545, 0.8800, 0.9167]\n",
    "rr_dim12_2_dil3_auc   = [0.7773, 0.7971, 0.8333]\n",
    "rr_dim12_2_dil3_f1    = [0.7059, 0.6667, 0.7500]\n",
    "\n",
    "rr_dim12_2_dil4_acc   = [0.8125, 0.8750, 0.9062]\n",
    "rr_dim12_2_dil4_bacc  = [0.7773, 0.8371, 0.8750]\n",
    "rr_dim12_2_dil4_spec  = [0.5000, 0.5714, 0.6250]\n",
    "rr_dim12_2_dil4_sens  = [0.9545, 0.9600, 1.0000]\n",
    "rr_dim12_2_dil4_auc   = [0.7273, 0.7657, 0.8125]\n",
    "rr_dim12_2_dil4_f1    = [0.6250, 0.6667, 0.7692]\n",
    "\n",
    "rr_dim12_2_dil5_acc   = [0.7500, 0.8750, 0.8750]\n",
    "rr_dim12_2_dil5_bacc  = [0.8136, 0.8371, 0.8958]\n",
    "rr_dim12_2_dil5_spec  = [0.8000, 0.5714, 0.7500]\n",
    "rr_dim12_2_dil5_sens  = [0.7273, 0.9600, 0.9167]\n",
    "rr_dim12_2_dil5_auc   = [0.7636, 0.7657, 0.8333]\n",
    "rr_dim12_2_dil5_f1    = [0.6667, 0.6667, 0.7500]\n",
    "\n",
    "rr_dim12_2_dil8_acc   = [0.8125, 0.8438, 0.8750]\n",
    "rr_dim12_2_dil8_bacc  = [0.7773, 0.9200, 0.8125]\n",
    "rr_dim12_2_dil8_spec  = [0.5000, 0.8571, 0.5000]\n",
    "rr_dim12_2_dil8_sens  = [0.9545, 0.8400, 1.0000]\n",
    "rr_dim12_2_dil8_auc   = [0.7273, 0.8486, 0.7500]\n",
    "rr_dim12_2_dil8_f1    = [0.6250, 0.7059, 0.6667]\n",
    "\n",
    "print(np.mean(rr_dim12_2_dil8_acc))\n",
    "print(np.mean(rr_dim12_2_dil8_bacc))\n",
    "print(np.mean(rr_dim12_2_dil8_spec))\n",
    "print(np.mean(rr_dim12_2_dil8_sens))\n",
    "print(np.mean(rr_dim12_2_dil8_auc))\n",
    "print(np.mean(rr_dim12_2_dil8_f1))\n",
    "print(\"===============\")\n",
    "print(np.std(rr_dim12_2_dil8_acc))\n",
    "print(np.std(rr_dim12_2_dil8_bacc))\n",
    "print(np.std(rr_dim12_2_dil8_spec))\n",
    "print(np.std(rr_dim12_2_dil8_sens))\n",
    "print(np.std(rr_dim12_2_dil8_auc))\n",
    "print(np.std(rr_dim12_2_dil8_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(5,5,5)\n",
    "b = np.random.rand(5,5,5)\n",
    "a = np.expand_dims(a, axis=0)\n",
    "b = np.expand_dims(b, axis=0)\n",
    "c = np.concatenate((a,b), axis=0)\n",
    "c = np.expand_dims(c, axis = 1)\n",
    "\n",
    "def gradual_outer_product(vec, interval):\n",
    "    num = int(np.ceil(len(vec)/interval))\n",
    "    res = [None] * num\n",
    "    start = 0\n",
    "    end = min(len(vec), interval)\n",
    "    for i in range(num):\n",
    "        outer_partial = np.outer(vec[start:end], vec)\n",
    "        res[i] = outer_partial.dot(vec)\n",
    "        start = start + interval\n",
    "        end = min(len(vec), start+interval)\n",
    "    res = np.concatenate(res, axis=0)\n",
    "    return res\n",
    "\n",
    "def sparse_attention_batch_NCWHD(data):\n",
    "    batch_size = data.shape[0]\n",
    "    res = [None] * batch_size\n",
    "    for idx in range(batch_size):\n",
    "        item = np.squeeze(data[idx])\n",
    "        num_nonzero = np.prod(item.shape) - np.sum(item == np.amin(item))\n",
    "        shifted_item = item - np.amin(item)\n",
    "        nonz_dim0, nonz_dim1, nonz_dim2 = np.nonzero(shifted_item)\n",
    "        assert(num_nonzero == len(nonz_dim0))\n",
    "        flatten_ = np.zeros(num_nonzero)\n",
    "        \n",
    "        for i in range(num_nonzero):\n",
    "            flatten_[i] = item[nonz_dim0[i]][nonz_dim1[i]][nonz_dim2[i]]\n",
    "        outer = gradual_outer_product(flatten_, 10)      \n",
    "        item_res = np.ones(item.shape) * np.amin(item)\n",
    "        for i in range(num_nonzero):\n",
    "            item_res[nonz_dim0[i]][nonz_dim1[i]][nonz_dim2[i]] = outer[i]\n",
    "        item_res = np.expand_dims(item_res, axis=0)\n",
    "        res[idx] = item_res\n",
    "    res = np.concatenate(res, axis=0)\n",
    "    res = np.expand_dims(res, axis=1)\n",
    "    return res\n",
    "\n",
    "# aa = np.array([1,2,3,4,5])\n",
    "# aa1 = np.outer(aa[0:3],aa)\n",
    "# aa2 = np.outer(aa[3:], aa)\n",
    "# aa1 = aa1.dot(aa)\n",
    "# aa2 = aa2.dot(aa)\n",
    "# print(np.concatenate((aa1, aa2), axis=0))\n",
    "\n",
    "res = sparse_attention_batch_NCWHD(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DMT.ipynb\n",
    "%run FileIO.ipynb\n",
    "%run viewer.ipynb\n",
    "%run Utility_general.ipynb\n",
    "\n",
    "path = 'E:/Data2/ArcGIS/Floor_CAD/Dataset/png/01_OfficeLab_01_F1_floorplan.png'\n",
    "img  = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "img[img != 255] = 0\n",
    "# img = 255 - img\n",
    "# img[img>45]  = 255\n",
    "# img[img<=45] = 0\n",
    "# img = 255 - img\n",
    "# viewer.imshow_(img)\n",
    "cnt, hry, red = Util_cv.compute_bnd_red_cv(img, 0, 255, 8)\n",
    "viewer.draw_red_on_single_dim_cv(red, component_to_plot=-1)\n",
    "#viewer.draw_bnd_on_single_dim_cv(img, cnt, (0, 255, 0), idx=-1)\n",
    "#print(\"Result: \", red[0])\n",
    "# m = Util_gen.fill_rift_(red[1], 8)\n",
    "# viewer.imshow_(m, 'jet')\n",
    "# print(np.amax(m), np.amin(m))\n",
    "\n",
    "\n",
    "# \n",
    "# imgplot = plt.imshow(img, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run viewer.ipynb\n",
    "%run viewer_3D.ipynb\n",
    "# ==================================================\n",
    "# PLOT BLOCK\n",
    "# ===== Persistence Threshold\n",
    "# voxel ratio in order of: 10.0, 5.5, 3.5, 2.0, 1.7, 0.7, 0.6, 0.2\n",
    "voxel_ratio_dim1  = [0.0817, 0.1331, 0.1785, 0.2295, 0.2420, 0.2906, 0.2962, 0.3206]\n",
    "voxel_ratio_dim2  = [0.1178, 0.1965, 0.2587, 0.3229, 0.3377, 0.3904, 0.3959, 0.4185]\n",
    "voxel_ratio_dim12 = [0.1852, 0.2984, 0.3866, 0.4740, 0.4936, 0.5611, 0.5680, 0.5960]\n",
    "pers_threshold   = [0.0843, 0.1754, 0.2792, 0.4352, 0.4819, 0.7088, 0.7406, 0.8960]\n",
    "acc_array  = [0.8437, 0.8437, 0.8229, 0.8511, 0.8541, 0.8511, 0.8229, 0.8269]\n",
    "sens_array = [0.9158, 0.8881, 0.8597, 0.9044, 0.9437, 0.9395, 0.8571, 0.8919]\n",
    "spec_array = [0.6464, 0.7238, 0.7154, 0.7365, 0.6071, 0.6470, 0.7214, 0.6756]\n",
    "f1_array   = [0.6825, 0.6999, 0.6729, 0.7437, 0.6805, 0.7128, 0.6825, 0.6987]\n",
    "auc_array  = [0.7811, 0.8060, 0.7875, 0.8204, 0.7754, 0.7932, 0.7893, 0.7837]\n",
    "\n",
    "viewer.draw_2D_line(voxel_ratio_dim12, acc_array, \"voxel ratio\", \"accuracy\")\n",
    "viewer.draw_2D_line(voxel_ratio_dim12, spec_array, \"voxel ratio\", \"specificity\")\n",
    "viewer.draw_2D_line(voxel_ratio_dim12, sens_array, \"voxel ratio\", \"sensitivity\")\n",
    "viewer.draw_2D_line(voxel_ratio_dim12, f1_array, \"voxel ratio\", \"f1 score\")\n",
    "viewer.draw_2D_line(voxel_ratio_dim12, auc_array, \"voxel ratio\", \"auc\")\n",
    "viewer.draw_2D_line(pers_threshold, acc_array, \"persistence threshold\", \"accuracy\")\n",
    "viewer.draw_2D_line(pers_threshold, spec_array, \"persistence threshold\", \"specificity\")\n",
    "viewer.draw_2D_line(pers_threshold, sens_array, \"persistence threshold\", \"sensitivity\")\n",
    "viewer.draw_2D_line(pers_threshold, f1_array, \"persistence threshold\", \"f1 score\")\n",
    "viewer.draw_2D_line(pers_threshold, auc_array, \"persistence threshold\", \"auc\")\n",
    "#viewer_3D.draw_3D_line(voxel_ratio_dim12, pers_threshold, acc_array, \"voxel ratio\", \"persistence threshold\", \"accuracy\")\n",
    "#viewer_3D.draw_3D_line(voxel_ratio_dim12, pers_threshold, spec_array, \"voxel ratio\", \"persistence threshold\", \"specificity\")\n",
    "#viewer_3D.draw_3D_line(voxel_ratio_dim12, pers_threshold, f1_array, \"voxel ratio\", \"persistence threshold\", \"f1 score\")\n",
    "\n",
    "# ===================================================\n",
    "# ===== Dilation Radius\n",
    "# voxel ratio in order of: 0, 1, 2, 3, 4, 5, 8\n",
    "# voxel_ratio_dim1  = [0.2295, 0.6197, 0.8446, 0.9328, 0.9678, 0.9834, 0.9971]\n",
    "# voxel_ratio_dim2  = [0.3229, 0.5945, 0.7482, 0.8412, 0.8980, 0.9335, 0.9800]\n",
    "# voxel_ratio_dim12 = [0.4740, 0.8017, 0.9104, 0.9558, 0.9774, 0.9882, 0.9980]\n",
    "# acc_array  = [0.8854, 0.8958, 0.8750, 0.8542, 0.8645, 0.8333, 0.8437]\n",
    "# spec_array = [0.7631, 0.6404, 0.8583, 0.6881, 0.5654, 0.7071, 0.6190]\n",
    "# sens_array = [0.9298, 0.9861, 0.8898, 0.9170, 0.9714, 0.8680, 0.9315]\n",
    "# f1_array   = [0.7749, 0.7591, 0.7778, 0.7075, 0.6869, 0.6944, 0.6658]\n",
    "# auc_array  = [0.8464, 0.8133, 0.8741, 0.8025, 0.7685, 0.7875, 0.7753]\n",
    "\n",
    "# viewer.draw_2D_line(voxel_ratio_dim12, acc_array, \"voxel ratio\", \"accuracy\")\n",
    "# viewer.draw_2D_line(voxel_ratio_dim12, spec_array, \"voxel ratio\", \"specificity\")\n",
    "# viewer.draw_2D_line(voxel_ratio_dim12, sens_array, \"voxel ratio\", \"sensitivity\")\n",
    "# viewer.draw_2D_line(voxel_ratio_dim12, f1_array, \"voxel ratio\", \"f1 score\")\n",
    "# viewer.draw_2D_line(voxel_ratio_dim12, auc_array, \"voxel ratio\", \"auc\")\n",
    "\n",
    "# ===== To do\n",
    "# 4. move to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image, deprocess_image\n",
    "from torchvision.models import resnet50\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_grad_cam import GuidedBackpropReLUModel\n",
    "\n",
    "%run viewer.ipynb\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "target_layer = model.layer4[-1].conv3\n",
    "#target_layer = model.avgpool\n",
    "#target_layer = model.fc\n",
    "#print(model)\n",
    "#print(target_layer)\n",
    "\n",
    "\n",
    "rgb_img = cv2.imread(\"E:/Data2/Dog_Cat/resnet_examples/both.png\", 1)[:, :, ::-1]\n",
    "rgb_img = np.float32(rgb_img) / 255\n",
    "input_tensor = preprocess_image(rgb_img, mean=[0.485, 0.456, 0.406], \n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "#Can be GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM\n",
    "cam = GradCAM(model=model, target_layer=target_layer, use_cuda=True)\n",
    "cam.batch_size = 32\n",
    "grayscale_cam = cam(input_tensor=input_tensor, target_category=None)\n",
    "cam_image = show_cam_on_image(rgb_img, grayscale_cam)\n",
    "\n",
    "gb_model = GuidedBackpropReLUModel(model=model, use_cuda=True)\n",
    "gb = gb_model(input_tensor, target_category=None)\n",
    "\n",
    "cam_mask = cv2.merge([grayscale_cam, grayscale_cam, grayscale_cam])\n",
    "cam_gb = deprocess_image(cam_mask * gb)\n",
    "gb = deprocess_image(gb)\n",
    "\n",
    "viewer.imshow_(rgb_img, gray=False)\n",
    "viewer.imshow_(cam_image, gray=False)\n",
    "viewer.imshow_(cam_gb, gray=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# input = torch.randn(16, 3, 224, 224)\n",
    "# model = resnet50(pretrained=True)\n",
    "# twist = nn.Sequential(*list(model.children())[:-1])\n",
    "# output = twist(input)\n",
    "# print(output.shape)\n",
    "\n",
    "# print(model)\n",
    "\n",
    "\n",
    "# m = nn.Linear(in_features=2048, out_features=1000, bias=True)\n",
    "# input = torch.randn(16, 2048, 1, 1)\n",
    "# input = torch.flatten(input, 1)\n",
    "# print(input.shape)\n",
    "# output = m(input)\n",
    "# print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run viewer.ipynb\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as tnn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCH = 20\n",
    "N_CLASSES = 2\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "                         std  = [ 0.229, 0.224, 0.225 ]),\n",
    "    ])\n",
    "\n",
    "trainData = dsets.ImageFolder('E:/Data2/Dog_Cat/train_subset', transform)\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(dataset=trainData, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "def conv_layer(chann_in, chann_out, k_size, p_size):\n",
    "    layer = tnn.Sequential(\n",
    "        tnn.Conv2d(chann_in, chann_out, kernel_size=k_size, padding=p_size),\n",
    "        tnn.BatchNorm2d(chann_out),\n",
    "        tnn.ReLU()\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "def vgg_conv_block(in_list, out_list, k_list, p_list, pooling_k, pooling_s):\n",
    "\n",
    "    layers = [ conv_layer(in_list[i], out_list[i], k_list[i], p_list[i]) for i in range(len(in_list)) ]\n",
    "    layers += [ tnn.MaxPool2d(kernel_size = pooling_k, stride = pooling_s)]\n",
    "    return tnn.Sequential(*layers)\n",
    "\n",
    "def vgg_fc_layer(size_in, size_out):\n",
    "    layer = tnn.Sequential(\n",
    "        tnn.Linear(size_in, size_out),\n",
    "        tnn.BatchNorm1d(size_out),\n",
    "        tnn.ReLU()\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "class VGG16(tnn.Module):\n",
    "    def __init__(self, n_classes=1000):\n",
    "        super(VGG16, self).__init__()\n",
    "\n",
    "        # Conv blocks (BatchNorm + ReLU activation added in each block)\n",
    "        self.layer1 = vgg_conv_block([3], [64], [3], [1], 2, 2)\n",
    "        self.layer2 = vgg_conv_block([64], [128], [3], [1], 2, 2)\n",
    "        self.layer3 = vgg_conv_block([128], [256], [3], [1], 2, 2)\n",
    "        self.layer4 = vgg_conv_block([256], [512], [3], [1], 2, 2)\n",
    "        self.layer5 = vgg_conv_block([512], [512], [3], [1], 2, 2)\n",
    "\n",
    "        # FC layers\n",
    "        self.layer6 = vgg_fc_layer(7*7*512, 4096)\n",
    "        self.layer7 = vgg_fc_layer(4096, 4096)\n",
    "\n",
    "        # Final layer\n",
    "        self.layer8 = tnn.Linear(4096, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        vgg16_features = self.layer5(out)\n",
    "        out = vgg16_features.view(out.size(0), -1)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "      \n",
    "# vgg16 = VGG16(n_classes=N_CLASSES)\n",
    "# vgg16 = vgg16.cuda()\n",
    "# vgg16.load_state_dict(torch.load('E:/Data2/Pytorch_Log/vgg16/cnn.pkl'))\n",
    "\n",
    "# # # Loss, Optimizer & Scheduler\n",
    "# # cost = tnn.CrossEntropyLoss()\n",
    "# # optimizer = torch.optim.SGD(vgg16.parameters(), lr=LEARNING_RATE)\n",
    "# # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# # # from medcam import medcam\n",
    "# # # vgg16 = medcam.inject(vgg16, output_dir='E:/Data2/gradcam_attention_maps', backend='ggcam', layer='layer5', label='best', save_maps=True)\n",
    "# # # vgg16.eval()\n",
    "\n",
    "# # # Train the model\n",
    "# # for epoch in range(EPOCH):\n",
    "\n",
    "# #     avg_loss = 0\n",
    "# #     cnt = 0\n",
    "# #     for images, labels in trainLoader:\n",
    "# #         images = images.cuda()\n",
    "# #         labels = labels.cuda()\n",
    "        \n",
    "# # #         img = images[0,:].unsqueeze(0)\n",
    "# # #         img_ = np.squeeze(img.cpu().numpy())\n",
    "# # #         img_ = np.moveaxis(img_, 0, -1)\n",
    "# # #         viewer.imshow_(img_)\n",
    "\n",
    "# #         # Forward + Backward + Optimize\n",
    "# #         optimizer.zero_grad()\n",
    "# #         outputs = vgg16(images)\n",
    "# #         loss = cost(outputs, labels)\n",
    "# #         avg_loss += loss.data\n",
    "# #         cnt += 1\n",
    "# #         print(\"[E: %d] loss: %f, avg_loss: %f\" % (epoch, loss.data, avg_loss/cnt))\n",
    "# #         loss.backward()\n",
    "# #         optimizer.step()\n",
    "# #     scheduler.step(avg_loss)\n",
    "    \n",
    "# # # Save the Trained Model\n",
    "# # torch.save(vgg16.state_dict(), 'E:/Data2/Pytorch_Log/vgg16/cnn.pkl')\n",
    "\n",
    "# # Test the model\n",
    "# vgg16.eval()\n",
    "# total = 0\n",
    "# correct = 0\n",
    "# for images, labels in trainLoader:\n",
    "#     images = images.cuda()\n",
    "#     labels = labels.numpy()\n",
    "#     outputs = vgg16(images)\n",
    "#     outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "#     total = total + outputs.shape[0]\n",
    "#     correct = correct + np.sum(outputs == labels)\n",
    "# print(correct * 1.0 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run Archpool.ipynb\n",
    "%run Argparser.ipynb\n",
    "%run FileIO.ipynb\n",
    "%run CNN.ipynb\n",
    "\n",
    "%run Cyclekernel.ipynb\n",
    "%run Medical_IO.ipynb\n",
    "%run Medical_Utility.ipynb\n",
    "\n",
    "%run GradCam_Utility.ipynb\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "settings    = return_settings()\n",
    "parser      = Argparser(settings)\n",
    "general     = return_general(0)\n",
    "arch1       = return_arch(18)\n",
    "#arch2       = return_arch(142)\n",
    "data_params = return_data_settings(0)\n",
    "\n",
    "cnn = CNN(general, [arch1])\n",
    "#cnn.train(data_params)\n",
    "cnn.test(data_params)\n",
    "#heatmap, vol = cnn.test(data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# This section loads in a synthetic raw VICTRE volume\n",
    "# and visualize it\n",
    "\n",
    "%run FileIO.ipynb\n",
    "%run viewer.ipynb\n",
    "%run viewer_3D.ipynb\n",
    "%run Medical_IO.ipynb\n",
    "%run Cyclekernel.ipynb\n",
    "%run Medical_Utility.ipynb\n",
    "\n",
    "label_dict = {0: \"background\", 1: \"fat\", 2: \"skin\", 29: \"glandular\", 33: \"nipple\", 40: \"muscle\", 88: \"ligament\", 95: \"TDLU\", 125: \"duct\", 150: \"artery\", 225: \"vein\"}\n",
    "\n",
    "def assign_normal_to_label(vol, T, label, loc, scale): \n",
    "    num_voxel = np.count_nonzero(vol == label)\n",
    "    num_list = np.random.normal(loc=loc, scale=scale/2.0, size=num_voxel)\n",
    "    h, w, d = vol.shape\n",
    "    cnt = 0\n",
    "    for i in range(0, h):\n",
    "        for j in range(0, w):\n",
    "            for k in range(0, d):\n",
    "                if vol[i][j][k] == label:\n",
    "                    T[i][j][k] = num_list[cnt]\n",
    "                    cnt = cnt + 1\n",
    "    return T\n",
    "\n",
    "def assign_T1_T2_breast_heterogeneous(vol, dtype_=np.float32):\n",
    "    '''\n",
    "    Assign T1 and T2 values to the breast volume, assuming homogeneous value for a breast part\n",
    "    @vol: input synthetic breast volume that has dtype: np.int32\n",
    "    Note: phatom voxel size is usually 0.25mm\n",
    "    '''\n",
    "    shape = vol.shape\n",
    "    T1 = np.zeros(shape, dtype=dtype_)\n",
    "    T2 = np.zeros(shape, dtype=dtype_)\n",
    "    # ===== T1 value =====\n",
    "    T1 = assign_normal_to_label(vol, T1, 1, 366.78, 7.75)     # label 1 fat\n",
    "    T1 = assign_normal_to_label(vol, T1, 2, 887.0, 92.0)      # label 2 skin\n",
    "    T1 = assign_normal_to_label(vol, T1, 29, 1444.83, 92.7)   # label 29 glandular\n",
    "    T1 = assign_normal_to_label(vol, T1, 33, 796.0, 21.0)     # label 33 nipple\n",
    "    T1 = assign_normal_to_label(vol, T1, 40, 1232.9, 255.0)   # label 40 muscle\n",
    "    T1 = assign_normal_to_label(vol, T1, 88, 400.0, 10.0)     # label 88 ligament (fake value)\n",
    "    T1 = assign_normal_to_label(vol, T1, 95, 1444.83, 92.7)   # label 95 TDLU\n",
    "    T1 = assign_normal_to_label(vol, T1, 125, 796.0, 21.0)    # label 125 duct\n",
    "    T1 = assign_normal_to_label(vol, T1, 150, 1984.4, 146.7)  # label 150 artery\n",
    "    T1 = assign_normal_to_label(vol, T1, 225, 1984.4, 146.7)  # label 225 vein\n",
    "    \n",
    "    # ===== T2 value =====\n",
    "    T2 = assign_normal_to_label(vol, T2, 1, 52.96, 1.54)\n",
    "    T2 = assign_normal_to_label(vol, T2, 2, 22.3, 7.0)\n",
    "    T2 = assign_normal_to_label(vol, T2, 29, 54.36, 9.35)\n",
    "    T2 = assign_normal_to_label(vol, T2, 33, 63.0, 4.0)\n",
    "    T2 = assign_normal_to_label(vol, T2, 40, 37.2, 9.8)\n",
    "    T2 = assign_normal_to_label(vol, T2, 88, 40.0, 2.0)\n",
    "    T2 = assign_normal_to_label(vol, T2, 95, 54.36, 9.35)\n",
    "    T2 = assign_normal_to_label(vol, T2, 125, 63.0, 4.0)\n",
    "    T2 = assign_normal_to_label(vol, T2, 150, 275.0, 0.0)\n",
    "    T2 = assign_normal_to_label(vol, T2, 225, 275.0, 0.0)\n",
    "    \n",
    "    return T1, T2\n",
    "\n",
    "def assign_T1_T2_breast_homogeneous(vol, dtype_=np.float32):\n",
    "    '''\n",
    "    Assign T1 and T2 values to the breast volume, assuming homogeneous value for a breast part\n",
    "    @vol: input synthetic breast volume that has dtype: np.int32\n",
    "    Note: phatom voxel size is usually 0.25mm\n",
    "    '''\n",
    "    shape = vol.shape\n",
    "    T1 = np.zeros(shape, dtype=dtype_)\n",
    "    T2 = np.zeros(shape, dtype=dtype_)\n",
    "    # ===== T1 value =====\n",
    "    T1[vol == 1]   = np.random.normal(loc=366.78, scale=7.75)   # label 1 fat\n",
    "    T1[vol == 2]   = np.random.normal(loc=887.0, scale=92.0)    # label 2 skin\n",
    "    T1[vol == 29]  = np.random.normal(loc=1444.83, scale=92.7)  # label 29 glandular\n",
    "    T1[vol == 33]  = np.random.normal(loc=796.0, scale=21.0)    # label 33 nipple\n",
    "    T1[vol == 40]  = np.random.normal(loc=1232.9, scale=255.0)  # label 40 muscle\n",
    "    T1[vol == 88]  = np.random.normal(loc=400.0, scale=10.0)    # label 88 ligament (fake value)\n",
    "    T1[vol == 95]  = np.random.normal(loc=1444.83, scale=92.7)  # label 95 TDLU\n",
    "    T1[vol == 125] = np.random.normal(loc=796.0, scale=21.0)    # label 125 duct\n",
    "    T1[vol == 150] = np.random.normal(loc=1984.4, scale=146.7)  # label 150 artery\n",
    "    T1[vol == 225] = np.random.normal(loc=1984.4, scale=146.7)  # label 225 vein\n",
    "    \n",
    "    # ===== T2 value =====\n",
    "    T2[vol == 1]   = np.random.normal(loc=52.96, scale=1.54)\n",
    "    T2[vol == 2]   = np.random.normal(loc=22.3, scale=7.0)\n",
    "    T2[vol == 29]  = np.random.normal(loc=54.36, scale=9.35)\n",
    "    T2[vol == 33]  = np.random.normal(loc=63.0, scale=4.0)\n",
    "    T2[vol == 40]  = np.random.normal(loc=37.2, scale=9.8)\n",
    "    T2[vol == 88]  = np.random.normal(loc=40.0, scale=2.0)\n",
    "    T2[vol == 95]  = np.random.normal(loc=54.36, scale=9.35)\n",
    "    T2[vol == 125] = np.random.normal(loc=63.0, scale=4.0)\n",
    "    T2[vol == 150] = np.random.normal(loc=275.0, scale=0.0)\n",
    "    T2[vol == 225] = np.random.normal(loc=275.0, scale=0.0)\n",
    "    return T1, T2\n",
    "\n",
    "def simulate_MRI_signal_dummy(volume, dtype_=np.float32):\n",
    "    '''\n",
    "    Assign fixed values to labels from on run of real simulation, only for test purpose.\n",
    "    '''\n",
    "    MRI_vol = np.zeros(volume.shape, dtype=dtype_)\n",
    "    MRI_vol[volume==1] = -0.18097298\n",
    "    MRI_vol[volume==2] = -0.10132162\n",
    "    MRI_vol[volume==29] = -0.10660842\n",
    "    MRI_vol[volume==33] = -0.1509192\n",
    "    MRI_vol[volume==40] = -0.11330113\n",
    "    MRI_vol[volume==88] = -0.16461924\n",
    "    MRI_vol[volume==95] = -0.105580494\n",
    "    MRI_vol[volume==125] = -0.1515352\n",
    "    MRI_vol[volume==150] = -0.10696436\n",
    "    MRI_vol[volume==225] = -0.11489603\n",
    "    return MRI_vol\n",
    "\n",
    "def simulate_MRI_signal(T1, T2, TR, TE, Alpha, dtype_=np.float32):\n",
    "    '''\n",
    "    Simulate MRI signal, assuming value 0.0 for air\n",
    "    @T1: input T1 volume\n",
    "    @T2: input T2 volume, same shape with T1\n",
    "    @TR: MR sequence parameter, use TR=50 for now\n",
    "    @TE: MR sequence parameter, use TE=10 for now\n",
    "    @Alpha: flip angle, use 6 degree for now\n",
    "    '''\n",
    "    shape = T1.shape\n",
    "    assert(shape == T2.shape)\n",
    "    MRI_vol = np.zeros(shape, dtype=dtype_)\n",
    "    MRI_vol[T1!=0.0] = np.sin(Alpha) * (1-np.exp(-TR/T1[T1!=0.0])) / (1-np.cos(Alpha) * np.exp(-TR/T1[T1!=0.0])) * np.exp(-TE/T2[T2!=0.0])\n",
    "    return MRI_vol\n",
    "\n",
    "def expand_by_one(chunk, fill_value):\n",
    "    '''\n",
    "    Expand 2D or 3D chunk by one pixel and fill the expanded part with fill_value\n",
    "    @chunk: input data\n",
    "    @fill_value: the value to be filled to the expanded part\n",
    "    '''\n",
    "    shape = chunk.shape\n",
    "    dim   = len(shape)\n",
    "    shape_expanded = [x+2 for x in shape]\n",
    "    chunk_expanded = np.ones(shape_expanded, dtype=chunk.dtype) * fill_value\n",
    "    if dim == 2:\n",
    "        chunk_expanded[1:shape_expanded[0]-1, 1:shape_expanded[1]-1] = chunk\n",
    "    elif dim == 3:\n",
    "        chunk_expanded[1:shape_expanded[0]-1, 1:shape_expanded[1]-1, 1:shape_expanded[2]-1] = chunk\n",
    "    else:\n",
    "        print(\"expand_by_one: Unsupported dimension\")\n",
    "    return chunk_expanded\n",
    "\n",
    "def chop_into_chunks(data, divide_factor, inverse, expand, min_or_max):\n",
    "    '''\n",
    "    This function divides large 3D volume or 2D image into small pieces.\n",
    "    @data: input data\n",
    "    @divide_factor: how many pieces the data is divided into for one dimension\n",
    "    @inverse: bool, if inverse the value of the chunk, used for superlevel set computation\n",
    "    @expand: bool, if to expand the chunk by one pixel\n",
    "    @min_or_max: use min or max to fill in the padded values, \"min\" or \"max\"\n",
    "    '''\n",
    "    shape = data.shape\n",
    "    dim   = len(shape)\n",
    "    divide_coord = []\n",
    "    for i in range(dim):\n",
    "        coord_ = [0]\n",
    "        cell_size = int(np.ceil(shape[i] / divide_factor))\n",
    "        while coord_[-1] != shape[i]:\n",
    "            coord_.append(np.minimum(coord_[-1]+cell_size, shape[i]))\n",
    "        divide_coord.append(coord_)\n",
    "        \n",
    "    chunks = []\n",
    "    if dim == 2:\n",
    "        for i in range(len(divide_coord[0])-1):\n",
    "            for j in range(len(divide_coord[1])-1):\n",
    "                sub_ = data[divide_coord[0][i]:divide_coord[0][i+1], divide_coord[1][j]:divide_coord[1][j+1]]\n",
    "                if (inverse):\n",
    "                    sub_ = -sub_\n",
    "                if (expand):\n",
    "                    if min_or_max == \"min\":\n",
    "                        sub_ = expand_by_one(sub_, np.amin(sub_))\n",
    "                    elif min_or_max == \"max\":\n",
    "                        sub_ = expand_by_one(sub_, np.amax(sub_))\n",
    "                    else:\n",
    "                        print(\"chop_into_chunks: Unsupported min_or_max mode\")\n",
    "                chunks.append(sub_)\n",
    "    elif dim == 3:\n",
    "        for i in range(len(divide_coord[0])-1):\n",
    "            for j in range(len(divide_coord[1])-1):\n",
    "                for k in range(len(divide_coord[2])-1):\n",
    "                    sub_ = data[divide_coord[0][i]:divide_coord[0][i+1], divide_coord[1][j]:divide_coord[1][j+1],\n",
    "                                divide_coord[2][k]:divide_coord[2][k+1]]\n",
    "                    if (inverse):\n",
    "                        sub_ = -sub_\n",
    "                    if (expand):\n",
    "                        if min_or_max == \"min\":\n",
    "                            sub_ = expand_by_one(sub_, np.amin(sub_))\n",
    "                        elif min_or_max == \"max\":\n",
    "                            sub_ = expand_by_one(sub_, np.amax(sub_))\n",
    "                        else:\n",
    "                            print(\"chop_into_chunks: Unsupported min_or_max mode\")\n",
    "                    chunks.append(sub_)\n",
    "    else:\n",
    "        print(\"chop_into_chunks: Unsupported dimension\")\n",
    "    return shape, chunks\n",
    "\n",
    "def resemble_chunks(chunks, shape, divide_factor, expand):\n",
    "    '''\n",
    "    Resemble chunks from function \"chop_into_chunks\"\n",
    "    @chunks: input data chunks\n",
    "    @shape: shape of the original data\n",
    "    @divide_factor: how many pieces the data is divided into for one dimension\n",
    "    @expand: bool, if chunk expanded, shrink it\n",
    "    '''\n",
    "    dim   = len(shape)\n",
    "    divide_coord = []\n",
    "    for i in range(dim):\n",
    "        coord_ = [0]\n",
    "        cell_size = int(np.ceil(shape[i] / divide_factor))\n",
    "        while coord_[-1] != shape[i]:\n",
    "            coord_.append(np.minimum(coord_[-1]+cell_size, shape[i]))\n",
    "        divide_coord.append(coord_)\n",
    "    data = np.zeros(shape, dtype=chunks[0].dtype)\n",
    "    cntr = 0\n",
    "    if dim == 2:\n",
    "        for i in range(len(divide_coord[0])-1):\n",
    "            for j in range(len(divide_coord[1])-1):\n",
    "                chunk_ = chunks[cntr]\n",
    "                if (expand):\n",
    "                    chunk_ = chunk_[1:chunk_.shape[0]-1, 1:chunk_.shape[1]-1]\n",
    "                data[divide_coord[0][i]:divide_coord[0][i+1], divide_coord[1][j]:divide_coord[1][j+1]] = chunk_\n",
    "                cntr = cntr + 1\n",
    "    elif dim == 3:\n",
    "        for i in range(len(divide_coord[0])-1):\n",
    "            for j in range(len(divide_coord[1])-1):\n",
    "                for k in range(len(divide_coord[2])-1):\n",
    "                    chunk_ = chunks[cntr]\n",
    "                    if (expand):\n",
    "                        chunk_ = chunk_[1:chunk_.shape[0]-1, 1:chunk_.shape[1]-1, 1:chunk_.shape[2]-1]\n",
    "                    data[divide_coord[0][i]:divide_coord[0][i+1], divide_coord[1][j]:divide_coord[1][j+1], divide_coord[2][k]:divide_coord[2][k+1]] = chunk_\n",
    "                    cntr = cntr + 1\n",
    "    else:\n",
    "        print(\"resemble_chunks: Unsupported dimension\")\n",
    "    return data\n",
    "\n",
    "def save_chunks(chunks, shape, divide_factor, direc):\n",
    "    '''\n",
    "    Save chunks from \"chop_into_chunks\"\n",
    "    '''\n",
    "    name_postfix = \"\"\n",
    "    for i in range(len(shape)-1):\n",
    "        name_postfix = name_postfix + str(shape[i]) + \"_\"\n",
    "    name_postfix = name_postfix + str(shape[-1]) + \"_\" + str(divide_factor) + \".dat\"\n",
    "        \n",
    "    for i in range(len(chunks)):\n",
    "        name = direc + \"/\" + str(i) + \"_\" + name_postfix\n",
    "        FileIO_MEDICAL.write_dat(chunks[i], name)\n",
    "        \n",
    "def read_topo_chunks(direc, shape, divide_factor, pers_thresh, mode):\n",
    "    '''\n",
    "    @direc: the directory where all topo results reside\n",
    "    @shape: shape of the original data\n",
    "    @divide_factor: how many pieces the data is divided into for one dimension\n",
    "    @pers_thresh: the persistence threshold to filter topo structures\n",
    "    @mode: string,\"persistence\" or \"birth\" or \"death\"\n",
    "    '''\n",
    "    dim   = len(shape)\n",
    "    chunk_num = int(pow(divide_factor, dim))\n",
    "    name_postfix = \"\"\n",
    "    for i in range(len(shape)-1):\n",
    "        name_postfix = name_postfix + str(shape[i]) + \"_\"\n",
    "    name_postfix = name_postfix + str(shape[-1]) + \"_\" + str(divide_factor) + \".dat\"\n",
    "    bnd = [None] * chunk_num\n",
    "    for i in range(chunk_num):\n",
    "        pers_name = direc + \"/\" + str(i) + \"_\" + name_postfix + \".pers.txt\"\n",
    "        bnd_name  = direc + \"/\" + str(i) + \"_\" + name_postfix + \".bnd\"\n",
    "        pers_ = FileIO_MEDICAL.read_pers_txt(pers_name)\n",
    "        bnd_  = FileIO_MEDICAL.read_bnd_red_unifieddim(bnd_name)\n",
    "        if mode == \"persistence\":\n",
    "            bnd_filt = Compute_CycleKernel.filter_bnd_or_red(bnd_, pers_, pers_thresh)\n",
    "        elif mode == \"birth\" or mode == \"death\":\n",
    "            bnd_filt = Compute_CycleKernel.filter_bnd_or_red_by_birth_or_death(bnd_, pers_, pers_thresh, mode)\n",
    "        else:\n",
    "            print(\"read_topo_chunks: invalid input parameters\")\n",
    "        bnd[i] = bnd_filt\n",
    "    return bnd\n",
    "\n",
    "def resemble_topo_chunks(dat, shape, divide_factor, drw_dim, expand):\n",
    "    '''\n",
    "    @dat: (filtered) boundary files computed with persistent homology and read by \"read_topo_chunks\"\n",
    "    @shape: shape of the original data\n",
    "    @divide_factor: how many pieces the data is divided into for one dimension\n",
    "    @drw_dim: dim to draw, can be 0, 1, or 2\n",
    "    @expand: bool, if chunk expanded, adjust topo structure coordinates\n",
    "    '''\n",
    "    dim = len(shape)\n",
    "    assert(drw_dim < dim)\n",
    "    divide_coord = []\n",
    "    for i in range(dim):\n",
    "        coord_ = [0]\n",
    "        cell_size = int(np.ceil(shape[i] / divide_factor))\n",
    "        while coord_[-1] != shape[i]:\n",
    "            coord_.append(np.minimum(coord_[-1]+cell_size, shape[i]))\n",
    "        divide_coord.append(coord_)\n",
    "    result = np.zeros(shape, dtype=np.int32)\n",
    "    \n",
    "    cnt = 0\n",
    "    if dim == 2:\n",
    "        for i in range(len(divide_coord[0])-1):\n",
    "            dim1_shift = divide_coord[0][i]\n",
    "            dim1_size = divide_coord[0][i+1] - divide_coord[0][i]\n",
    "            for j in range(len(divide_coord[1])-1):\n",
    "                dim2_shift = divide_coord[1][j]\n",
    "                dim2_size = divide_coord[1][j+1] - divide_coord[1][j]\n",
    "                assert(drw_dim < len(dat[cnt]))\n",
    "                for l in range(len(dat[cnt][drw_dim])):\n",
    "                    for m in range(dat[cnt][drw_dim][l].shape[1]):\n",
    "                        dim1_coord = dat[cnt][drw_dim][l][1,m]\n",
    "                        dim2_coord = dat[cnt][drw_dim][l][0,m]\n",
    "                        if (expand):\n",
    "                            if (dim1_coord == 0 or dim2_coord == 0 or\n",
    "                            dim1_coord == dim1_size+1 or dim2_coord == dim2_size+1):\n",
    "                                continue\n",
    "                            dim1_coord = dim1_coord + dim1_shift - 1\n",
    "                            dim2_coord = dim2_coord + dim2_shift - 1\n",
    "                        else:\n",
    "                            dim1_coord = dim1_coord + dim1_shift\n",
    "                            dim2_coord = dim2_coord + dim2_shift\n",
    "                        result[dim1_coord][dim2_coord] = 1\n",
    "                cnt = cnt + 1\n",
    "    elif dim == 3:\n",
    "        for i in range(len(divide_coord[0])-1):\n",
    "            dim1_shift = divide_coord[0][i]\n",
    "            dim1_size = divide_coord[0][i+1] - divide_coord[0][i]\n",
    "            for j in range(len(divide_coord[1])-1):\n",
    "                dim2_shift = divide_coord[1][j]\n",
    "                dim2_size = divide_coord[1][j+1] - divide_coord[1][j]\n",
    "                for k in range(len(divide_coord[2])-1):\n",
    "                    dim3_shift = divide_coord[2][k]\n",
    "                    dim3_size = divide_coord[2][k+1] - divide_coord[2][k]\n",
    "                    assert(drw_dim < len(dat[cnt]))\n",
    "                    for l in range(len(dat[cnt][drw_dim])):\n",
    "                        for m in range(dat[cnt][drw_dim][l].shape[1]):\n",
    "                            dim1_coord = dat[cnt][drw_dim][l][2,m]\n",
    "                            dim2_coord = dat[cnt][drw_dim][l][1,m]\n",
    "                            dim3_coord = dat[cnt][drw_dim][l][0,m]\n",
    "                            if (expand):\n",
    "                                if (dim1_coord == 0 or dim2_coord == 0 or dim3_coord == 0 or\n",
    "                                dim1_coord == dim1_size+1 or dim2_coord == dim2_size+1 or dim3_coord == dim3_size+1):\n",
    "                                    continue\n",
    "                                dim1_coord = dim1_coord + dim1_shift - 1\n",
    "                                dim2_coord = dim2_coord + dim2_shift - 1\n",
    "                                dim3_coord = dim3_coord + dim3_shift - 1\n",
    "                            else:\n",
    "                                dim1_coord = dim1_coord + dim1_shift\n",
    "                                dim2_coord = dim2_coord + dim2_shift\n",
    "                                dim3_coord = dim3_coord + dim3_shift\n",
    "                            result[dim1_coord][dim2_coord][dim3_coord] = 1\n",
    "                    cnt = cnt + 1\n",
    "    else:\n",
    "        print(\"resemble_topo_chunks: Unsupported dimension\")\n",
    "    return result\n",
    "\n",
    "def compute_percentage_VICTRE(bnd_draw, volume):\n",
    "    '''\n",
    "    @bnd_draw: binary mask indicating the topo structures\n",
    "    @volume: integer mask indicating different breast parts starting from 0\n",
    "    @rec_tp_percentage: #overlapping voxels / #total topo voxels\n",
    "    @rec_rc_percentage: #overlapping voxels / #breast part voxels\n",
    "    '''\n",
    "    volume_backup = volume.copy()\n",
    "    total = np.sum(bnd_draw==1)\n",
    "    volume[bnd_draw != 1] = -1\n",
    "    rec_tp_percentage = [0.0] * len(np.unique(volume))\n",
    "    rec_rc_percentage = [0.0] * len(np.unique(volume))\n",
    "    for i in range(len(np.unique(volume))):\n",
    "        if np.unique(volume)[i] == -1:\n",
    "            continue\n",
    "        rec_tp_percentage[i] = np.sum(volume == np.unique(volume)[i]) / total\n",
    "        rec_rc_percentage[i] = np.sum(volume == np.unique(volume)[i]) / np.sum(volume_backup == np.unique(volume)[i])\n",
    "    print(\"#overlapping voxels / #total topo voxels\")\n",
    "    for i in range(len(np.unique(volume))):\n",
    "        if np.unique(volume)[i] == -1:\n",
    "            continue\n",
    "        print(label_dict[np.unique(volume)[i]], \"{:.2f}\".format(rec_tp_percentage[i] * 100) + \"%\")\n",
    "    print(\"------------------------------\")\n",
    "    print(\"#overlapping voxels / #breast part voxels\")\n",
    "    for i in range(len(np.unique(volume))):\n",
    "        if np.unique(volume)[i] == -1:\n",
    "            continue\n",
    "        print(label_dict[np.unique(volume)[i]], \"{:.2f}\".format(rec_rc_percentage[i] * 100) + \"%\")\n",
    "        \n",
    "def show_basic_info(file_path, volume, MRI_):\n",
    "    print(file_path)\n",
    "    volume_voxel_num = np.sum(volume != 0)\n",
    "    gt_breast_part_percentage = [0.0] * (len(np.unique(volume))-1)\n",
    "    for i in range(len(np.unique(volume))-1):\n",
    "        gt_breast_part_percentage[i] = np.sum(volume == np.unique(volume)[i+1]) / volume_voxel_num\n",
    "    print(\"Ground Truth Breast Part Percentage\")\n",
    "    for i in range(len(np.unique(volume))-1):\n",
    "        print(np.unique(volume)[i+1], gt_breast_part_percentage[i])\n",
    "\n",
    "    MRI_neg = -MRI_\n",
    "    print(\"Printing negative simulated MRI signal values\")\n",
    "    for i in range(len(np.unique(volume))-1):\n",
    "        print(np.unique(volume)[i+1], np.mean(MRI_neg[volume == np.unique(volume)[i+1]]))\n",
    "        \n",
    "def dd():\n",
    "    file_path = [None] * 5\n",
    "    file_path[0] = \"E:/Data2/VICTRE/synthetic_data/dense_250um/dense_250um_01/p_715565079.raw\"\n",
    "    file_path[1] = \"E:/Data2/VICTRE/synthetic_data/dense_250um/dense_250um_02/p_1054762632.raw\"\n",
    "    file_path[2] = \"E:/Data2/VICTRE/synthetic_data/dense_250um/dense_250um_03/p_1058544579.raw\"\n",
    "    file_path[3] = \"E:/Data2/VICTRE/synthetic_data/dense_250um/dense_250um_04/p_1820926860.raw\"\n",
    "    file_path[4] = \"E:/Data2/VICTRE/synthetic_data/dense_250um/dense_250um_05/p_-1202882643.raw\"\n",
    "    gt_breast_part_total = [0.0] * 100\n",
    "    for i in range(5):\n",
    "        volume = FileIO.read_VICTRE_raw(file_path[i])\n",
    "        volume_voxel_num = np.sum(volume != 0)\n",
    "        gt_breast_part_percentage = [0.0] * (len(np.unique(volume))-1)\n",
    "        for i in range(len(np.unique(volume))-1):\n",
    "            gt_breast_part_percentage[i] = np.sum(volume == np.unique(volume)[i+1]) / volume_voxel_num\n",
    "            gt_breast_part_total[i] = gt_breast_part_total[i] + gt_breast_part_percentage[i]\n",
    "\n",
    "    volume = FileIO.read_VICTRE_raw(file_path[0])\n",
    "    print(\"Ground Truth Breast Part Percentage\")\n",
    "    for i in range(len(np.unique(volume))-1):\n",
    "        print(np.unique(volume)[i+1], gt_breast_part_total[i]/5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = \"fatty\"\n",
    "#file_path1 = \"E:/Data2/VICTRE/Sample_phantom/Dense/dense_p_-321964974.raw\"\n",
    "file_path2 = \"E:/Data2/VICTRE/synthetic_data/\"+config+\"_250um/\"+config+\"_250um_01/p_2119279428.raw\"\n",
    "volume, shape = FileIO.read_VICTRE_raw(file_path2)\n",
    "#T1, T2 = assign_T1_T2_breast_homogeneous(volume)\n",
    "#T1, T2 = assign_T1_T2_breast_heterogeneous(volume)\n",
    "#MRI_ = simulate_MRI_signal(T1, T2, 50, 10, 6)\n",
    "# MRI_ = simulate_MRI_signal_dummy(volume)\n",
    "# MRI_ = Util_gen.downscale_image_ND(MRI_, (4,4,4), np.mean)\n",
    "# show_basic_info(file_path2, volume, MRI_)\n",
    "\n",
    "\n",
    "\n",
    "# volume[volume!=29] = 0\n",
    "# volume[volume==29] = 1\n",
    "\n",
    "\n",
    "# shape, chunks = chop_into_chunks(MRI_, 3, True, True, \"min\")359, 359, 299\n",
    "# vol_ = resemble_chunks(chunks, shape, 3, True)\n",
    "# save_chunks(chunks, shape, 3, 'E:/Data2/VICTRE/synthetic_data/scattered_250um/data_scattered_250um_01/dfactor3_padmin_dummysimu_inverse')\n",
    "bnd1 = read_topo_chunks('E:/Data2/VICTRE/synthetic_data/'+config+'_250um/data_'+config+'_250um_01/dfactor3_padmin_dummysimu_inverse', shape, 3, 0.0975, \"birth\")\n",
    "bnd_draw_d1 = resemble_topo_chunks(bnd1, shape, 3, 1, True)\n",
    "bnd_draw_d2 = resemble_topo_chunks(bnd1, shape, 3, 2, True)\n",
    "bnd_draw = bnd_draw_d1 | bnd_draw_d2\n",
    "# bnd2 = read_topo_chunks('E:/Data2/VICTRE/synthetic_data/scattered_250um/data_scattered_250um_01/dfactor3_padmin_dummysimu_inverse', [434, 446, 384], 3, 0.1067, \"birth\")\n",
    "# bnd_draw2 = resemble_topo_chunks(bnd2, [434, 446, 384], 3, 1, True)\n",
    "# bnd_draw = bnd_draw1 - bnd_draw2\n",
    "compute_percentage_VICTRE(bnd_draw, volume)\n",
    "\n",
    "# # #volume[volume!=1] = 0\n",
    "# # volume = Util_gen.downscale_image_ND(volume, (4,4,4), Util_gen.rand_)\n",
    "# # #volume, _ = assign_T1_T2_breast_homogeneous(volume)\n",
    "# # #volume = Util_gen.downscale_image_ND(volume, (4,4,4), np.mean)\n",
    "# # FileIO_MEDICAL.write_dat(MRI_, 'E:/Data2/VICTRE/synthetic_data/data_scattered_250um_01/p_1063873867.dat')\n",
    "# # # viewer_3D.tmp_viewer(volume, opacity=0.5)\n",
    "\n",
    "# # ===== Visualization of persistent homology =====\n",
    "# vol  = FileIO_MEDICAL.read_dat(\"E:/Data2/VICTRE/synthetic_data/data_scattered_250um_01/dfactor3_padmin_wSimulation/14_434_446_384_2.dat\")\n",
    "# pers = FileIO_MEDICAL.read_pers_txt('E:/Data2/VICTRE/synthetic_data/data_scattered_250um_01/dfactor3_padmin_wSimulation/14_434_446_384_2.dat.pers.txt')\n",
    "# bnd  = FileIO_MEDICAL.read_bnd_red_unifieddim('E:/Data2/VICTRE/synthetic_data/data_scattered_250um_01/dfactor3_padmin_wSimulation/14_434_446_384_2.dat.bnd')\n",
    "# bnd_filt = Compute_CycleKernel.filter_bnd_or_red(bnd, pers, 0.0)\n",
    "# #bnd_drw = Utility_MEDICAL.draw_on_volume_ori_intensity(bnd_filt, vol, seg, tum, 10, 0, 1, np.arange(0,len(bnd_filt[1])), [500,1500])\n",
    "# #bnd_drw2 = Utility_MEDICAL.draw_on_volume_ori_intensity(bnd_filt, vol, seg, tum, 10, 0, 2, np.arange(0,len(bnd_filt[2])), [800,1200])\n",
    "# bnd_draw = Utility_MEDICAL.draw_on_volume(bnd_filt, vol.shape, 1, np.arange(0,len(bnd_filt[1])))\n",
    "\n",
    "# bnd_draw = bnd_draw[1:146,1:150,1:129] \n",
    "\n",
    "#volume[volume != 29] = 0\n",
    "\n",
    "\n",
    "#viewer_3D.tmp2_viewer(volume, bnd_draw_d1)\n",
    "#viewer_3D.tmp_viewer(volume, opacity=0.5)\n",
    "\n",
    "#FileIO_MEDICAL.save_nii(bnd_draw_d2, \"E:/Data2/MEDIA_visualization/dense_01_topo2d.nii\")\n",
    "#MRI_ = FileIO_MEDICAL.load_nii(\"E:/Data2/VICTRE/synthetic_data/data_scattered_250um_01/p_1063873867_MRI.nii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volume shape:  [287, 359, 202]\n",
      "[dense] results:\n",
      "#overlapping voxels / #total topo voxels\n",
      "background 0.23%\n",
      "skin 5.12%\n",
      "glandular 90.20%\n",
      "nipple 0.03%\n",
      "muscle 0.20%\n",
      "ligament 0.41%\n",
      "TDLU 1.11%\n",
      "duct 0.57%\n",
      "artery 0.92%\n",
      "vein 1.21%\n",
      "-------------------------------------------------------------\n",
      "#overlapping voxels / #breast part voxels\n",
      "background 0.02%\n",
      "skin 13.50%\n",
      "glandular 24.43%\n",
      "nipple 2.67%\n",
      "muscle 0.10%\n",
      "ligament 3.45%\n",
      "TDLU 40.42%\n",
      "duct 29.46%\n",
      "artery 30.39%\n",
      "vein 29.94%\n",
      "*************************************************************\n",
      "[Nearest distance from tissue to topo] min 1d 2d 12d:  0.0 0.0 0.0 max 1d 2d 12d:  30.59411708155671 37.416573867739416 30.59411708155671 mid 1d 2d 12d:  3.605551275463989 2.0 1.4142135623730951 mean 1d 2d 12d:  4.143140311388984 2.723890220724306 2.0244263129155837\n",
      "[Nearest distance from topo to tissue] min 1d 2d 12d:  0.0 0.0 0.0 max 1d 2d 12d:  28.0178514522438 47.391982444291145 47.391982444291145 mid 1d 2d 12d:  0.0 0.0 0.0 mean 1d 2d 12d:  0.23644567477351255 2.2373088938199315 2.0964276964222033\n",
      "=============================================================\n",
      "=============================================================\n",
      "volume shape:  [359, 359, 299]\n",
      "[hetero] results:\n",
      "#overlapping voxels / #total topo voxels\n",
      "background 0.67%\n",
      "skin 0.26%\n",
      "glandular 95.54%\n",
      "nipple 0.01%\n",
      "muscle 0.07%\n",
      "ligament 0.53%\n",
      "TDLU 0.24%\n",
      "duct 0.42%\n",
      "artery 1.10%\n",
      "vein 1.16%\n",
      "-------------------------------------------------------------\n",
      "#overlapping voxels / #breast part voxels\n",
      "background 0.03%\n",
      "skin 0.40%\n",
      "glandular 20.06%\n",
      "nipple 0.56%\n",
      "muscle 0.03%\n",
      "ligament 2.07%\n",
      "TDLU 10.53%\n",
      "duct 20.10%\n",
      "artery 19.67%\n",
      "vein 18.00%\n",
      "*************************************************************\n",
      "[Nearest distance from tissue to topo] min 1d 2d 12d:  0.0 0.0 0.0 max 1d 2d 12d:  42.59107887809371 59.43904440685432 31.20897306865447 mid 1d 2d 12d:  2.449489742783178 3.1622776601683795 1.4142135623730951 mean 1d 2d 12d:  2.994045332406743 4.327135240005904 2.033898076751475\n",
      "[Nearest distance from topo to tissue] min 1d 2d 12d:  0.0 0.0 0.0 max 1d 2d 12d:  39.56008088970496 66.06814663663572 66.06814663663572 mid 1d 2d 12d:  0.0 0.0 0.0 mean 1d 2d 12d:  0.3311348393240688 6.965846312402442 5.51963133736698\n",
      "=============================================================\n",
      "=============================================================\n",
      "volume shape:  [434, 446, 384]\n",
      "[scattered] results:\n",
      "#overlapping voxels / #total topo voxels\n",
      "background 0.03%\n",
      "skin 1.56%\n",
      "glandular 93.74%\n",
      "nipple 0.00%\n",
      "muscle 0.26%\n",
      "ligament 0.87%\n",
      "TDLU 0.36%\n",
      "duct 0.59%\n",
      "artery 1.28%\n",
      "vein 1.30%\n",
      "-------------------------------------------------------------\n",
      "#overlapping voxels / #breast part voxels\n",
      "background 0.00%\n",
      "skin 1.21%\n",
      "glandular 19.13%\n",
      "nipple 0.29%\n",
      "muscle 0.06%\n",
      "ligament 1.35%\n",
      "TDLU 22.18%\n",
      "duct 23.83%\n",
      "artery 18.66%\n",
      "vein 16.44%\n",
      "*************************************************************\n",
      "[Nearest distance from tissue to topo] min 1d 2d 12d:  0.0 0.0 0.0 max 1d 2d 12d:  45.221676218380054 72.92461861401813 39.824615503479755 mid 1d 2d 12d:  2.0 5.196152422706632 1.4142135623730951 mean 1d 2d 12d:  2.489751119262768 6.811100997144482 2.0176516032294725\n",
      "[Nearest distance from topo to tissue] min 1d 2d 12d:  0.0 0.0 0.0 max 1d 2d 12d:  68.52736679604726 83.64807230295268 83.64807230295268 mid 1d 2d 12d:  0.0 17.74823934929885 0.0 mean 1d 2d 12d:  0.6019958896881449 16.58109955215949 11.545061216853172\n",
      "=============================================================\n",
      "=============================================================\n",
      "volume shape:  [440, 518, 488]\n",
      "[fatty] results:\n",
      "#overlapping voxels / #total topo voxels\n",
      "background 0.13%\n",
      "skin 2.38%\n",
      "glandular 92.55%\n",
      "nipple 0.01%\n",
      "muscle 0.63%\n",
      "ligament 1.54%\n",
      "TDLU 0.10%\n",
      "duct 1.06%\n",
      "artery 0.75%\n",
      "vein 0.85%\n",
      "-------------------------------------------------------------\n",
      "#overlapping voxels / #breast part voxels\n",
      "background 0.00%\n",
      "skin 1.15%\n",
      "glandular 21.91%\n",
      "nipple 0.53%\n",
      "muscle 0.11%\n",
      "ligament 1.34%\n",
      "TDLU 12.49%\n",
      "duct 21.65%\n",
      "artery 15.74%\n",
      "vein 14.44%\n",
      "*************************************************************\n",
      "[Nearest distance from tissue to topo] min 1d 2d 12d:  0.0 0.0 0.0 max 1d 2d 12d:  42.720018726587654 85.05292469985967 37.33630940518894 mid 1d 2d 12d:  1.4142135623730951 7.810249675906654 1.4142135623730951 mean 1d 2d 12d:  2.0512111044122983 9.72540176640149 1.8289822187730442\n",
      "[Nearest distance from topo to tissue] min 1d 2d 12d:  0.0 0.0 0.0 max 1d 2d 12d:  90.21086409075129 91.08786966440702 91.08786966440702 mid 1d 2d 12d:  0.0 23.790754506740637 12.84523257866513 mean 1d 2d 12d:  0.895318773867908 22.154885664827603 14.776683393909408\n",
      "=============================================================\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "%run FileIO.ipynb\n",
    "%run viewer.ipynb\n",
    "%run viewer_3D.ipynb\n",
    "%run Medical_IO.ipynb\n",
    "%run Cyclekernel.ipynb\n",
    "%run Medical_Utility.ipynb\n",
    "\n",
    "label_dict = {0: \"background\", 1: \"fat\", 2: \"skin\", 29: \"glandular\", 33: \"nipple\", 40: \"muscle\", 88: \"ligament\", 95: \"TDLU\", 125: \"duct\", 150: \"artery\", 225: \"vein\"}\n",
    "\n",
    "def read_topo_chunks(direc, shape, divide_factor, pers_thresh, mode):\n",
    "    '''\n",
    "    @direc: the directory where all topo results reside\n",
    "    @shape: shape of the original data\n",
    "    @divide_factor: how many pieces the data is divided into for one dimension\n",
    "    @pers_thresh: the persistence threshold to filter topo structures\n",
    "    @mode: string,\"persistence\" or \"birth\" or \"death\"\n",
    "    '''\n",
    "    dim   = len(shape)\n",
    "    chunk_num = int(pow(divide_factor, dim))\n",
    "    name_postfix = \"\"\n",
    "    for i in range(len(shape)-1):\n",
    "        name_postfix = name_postfix + str(shape[i]) + \"_\"\n",
    "    name_postfix = name_postfix + str(shape[-1]) + \"_\" + str(divide_factor) + \".dat\"\n",
    "    bnd = [None] * chunk_num\n",
    "    for i in range(chunk_num):\n",
    "        pers_name = direc + \"/\" + str(i) + \"_\" + name_postfix + \".pers.txt\"\n",
    "        bnd_name  = direc + \"/\" + str(i) + \"_\" + name_postfix + \".bnd\"\n",
    "        pers_ = FileIO_MEDICAL.read_pers_txt(pers_name)\n",
    "        bnd_  = FileIO_MEDICAL.read_bnd_red_unifieddim(bnd_name)\n",
    "        if mode == \"persistence\":\n",
    "            bnd_filt = Compute_CycleKernel.filter_bnd_or_red(bnd_, pers_, pers_thresh)\n",
    "        elif mode == \"birth\" or mode == \"death\":\n",
    "            bnd_filt = Compute_CycleKernel.filter_bnd_or_red_by_birth_or_death(bnd_, pers_, pers_thresh, mode)\n",
    "        else:\n",
    "            print(\"read_topo_chunks: invalid input parameters\")\n",
    "        bnd[i] = bnd_filt\n",
    "    return bnd\n",
    "\n",
    "def resemble_topo_chunks(dat, shape, divide_factor, drw_dim, expand):\n",
    "    '''\n",
    "    @dat: (filtered) boundary files computed with persistent homology and read by \"read_topo_chunks\"\n",
    "    @shape: shape of the original data\n",
    "    @divide_factor: how many pieces the data is divided into for one dimension\n",
    "    @drw_dim: dim to draw, can be 0, 1, or 2\n",
    "    @expand: bool, if chunk expanded, adjust topo structure coordinates\n",
    "    '''\n",
    "    dim = len(shape)\n",
    "    assert(drw_dim < dim)\n",
    "    divide_coord = []\n",
    "    for i in range(dim):\n",
    "        coord_ = [0]\n",
    "        cell_size = int(np.ceil(shape[i] / divide_factor))\n",
    "        while coord_[-1] != shape[i]:\n",
    "            coord_.append(np.minimum(coord_[-1]+cell_size, shape[i]))\n",
    "        divide_coord.append(coord_)\n",
    "    result = np.zeros(shape, dtype=np.int32)\n",
    "    \n",
    "    cnt = 0\n",
    "    if dim == 2:\n",
    "        for i in range(len(divide_coord[0])-1):\n",
    "            dim1_shift = divide_coord[0][i]\n",
    "            dim1_size = divide_coord[0][i+1] - divide_coord[0][i]\n",
    "            for j in range(len(divide_coord[1])-1):\n",
    "                dim2_shift = divide_coord[1][j]\n",
    "                dim2_size = divide_coord[1][j+1] - divide_coord[1][j]\n",
    "                assert(drw_dim < len(dat[cnt]))\n",
    "                for l in range(len(dat[cnt][drw_dim])):\n",
    "                    for m in range(dat[cnt][drw_dim][l].shape[1]):\n",
    "                        dim1_coord = dat[cnt][drw_dim][l][1,m]\n",
    "                        dim2_coord = dat[cnt][drw_dim][l][0,m]\n",
    "                        if (expand):\n",
    "                            if (dim1_coord == 0 or dim2_coord == 0 or\n",
    "                            dim1_coord == dim1_size+1 or dim2_coord == dim2_size+1):\n",
    "                                continue\n",
    "                            dim1_coord = dim1_coord + dim1_shift - 1\n",
    "                            dim2_coord = dim2_coord + dim2_shift - 1\n",
    "                        else:\n",
    "                            dim1_coord = dim1_coord + dim1_shift\n",
    "                            dim2_coord = dim2_coord + dim2_shift\n",
    "                        result[dim1_coord][dim2_coord] = 1\n",
    "                cnt = cnt + 1\n",
    "    elif dim == 3:\n",
    "        for i in range(len(divide_coord[0])-1):\n",
    "            dim1_shift = divide_coord[0][i]\n",
    "            dim1_size = divide_coord[0][i+1] - divide_coord[0][i]\n",
    "            for j in range(len(divide_coord[1])-1):\n",
    "                dim2_shift = divide_coord[1][j]\n",
    "                dim2_size = divide_coord[1][j+1] - divide_coord[1][j]\n",
    "                for k in range(len(divide_coord[2])-1):\n",
    "                    dim3_shift = divide_coord[2][k]\n",
    "                    dim3_size = divide_coord[2][k+1] - divide_coord[2][k]\n",
    "                    assert(drw_dim < len(dat[cnt]))\n",
    "                    for l in range(len(dat[cnt][drw_dim])):\n",
    "                        for m in range(dat[cnt][drw_dim][l].shape[1]):\n",
    "                            dim1_coord = dat[cnt][drw_dim][l][2,m]\n",
    "                            dim2_coord = dat[cnt][drw_dim][l][1,m]\n",
    "                            dim3_coord = dat[cnt][drw_dim][l][0,m]\n",
    "                            if (expand):\n",
    "                                if (dim1_coord == 0 or dim2_coord == 0 or dim3_coord == 0 or\n",
    "                                dim1_coord == dim1_size+1 or dim2_coord == dim2_size+1 or dim3_coord == dim3_size+1):\n",
    "                                    continue\n",
    "                                dim1_coord = dim1_coord + dim1_shift - 1\n",
    "                                dim2_coord = dim2_coord + dim2_shift - 1\n",
    "                                dim3_coord = dim3_coord + dim3_shift - 1\n",
    "                            else:\n",
    "                                dim1_coord = dim1_coord + dim1_shift\n",
    "                                dim2_coord = dim2_coord + dim2_shift\n",
    "                                dim3_coord = dim3_coord + dim3_shift\n",
    "                            result[dim1_coord][dim2_coord][dim3_coord] = 1\n",
    "                    cnt = cnt + 1\n",
    "    else:\n",
    "        print(\"resemble_topo_chunks: Unsupported dimension\")\n",
    "    return result\n",
    "\n",
    "def compute_percentage_VICTRE(bnd_draw, volume_):\n",
    "    '''\n",
    "    @bnd_draw: binary mask indicating the topo structures\n",
    "    @volume: integer mask indicating different breast parts starting from 0\n",
    "    @rec_tp_percentage: #overlapping voxels / #total topo voxels\n",
    "    @rec_rc_percentage: #overlapping voxels / #breast part voxels\n",
    "    '''\n",
    "    volume = volume_.copy()\n",
    "    volume_backup = volume_.copy()\n",
    "    total = np.sum(bnd_draw==1)\n",
    "    volume[bnd_draw != 1] = -1\n",
    "    rec_tp_percentage = [0.0] * len(np.unique(volume))\n",
    "    rec_rc_percentage = [0.0] * len(np.unique(volume))\n",
    "    for i in range(len(np.unique(volume))):\n",
    "        if np.unique(volume)[i] == -1:\n",
    "            continue\n",
    "        rec_tp_percentage[i] = np.sum(volume == np.unique(volume)[i]) / total\n",
    "        rec_rc_percentage[i] = np.sum(volume == np.unique(volume)[i]) / np.sum(volume_backup == np.unique(volume)[i])\n",
    "    print(\"#overlapping voxels / #total topo voxels\")\n",
    "    for i in range(len(np.unique(volume))):\n",
    "        if np.unique(volume)[i] == -1:\n",
    "            continue\n",
    "        print(label_dict[np.unique(volume)[i]], \"{:.2f}\".format(rec_tp_percentage[i] * 100) + \"%\")\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    print(\"#overlapping voxels / #breast part voxels\")\n",
    "    for i in range(len(np.unique(volume))):\n",
    "        if np.unique(volume)[i] == -1:\n",
    "            continue\n",
    "        print(label_dict[np.unique(volume)[i]], \"{:.2f}\".format(rec_rc_percentage[i] * 100) + \"%\")\n",
    "        \n",
    "def dist_gland2topo(volume, bnd, merge_gland):\n",
    "    '''\n",
    "    Compute the distance from glandular voxels to nearest topo structures.\n",
    "    volume and bnd are both binary mask.\n",
    "    '''\n",
    "    import scipy.ndimage\n",
    "    v = volume.copy()\n",
    "    b = 1 - bnd.copy()\n",
    "    if (merge_gland):\n",
    "        label_list = [29, 95, 125, 150, 225]      # Correspond to Glandular, TDLU, Duct, Artery, Vein resp.\n",
    "        for n in label_list:\n",
    "            v[v == n] = -1\n",
    "        v[v != -1] = 0\n",
    "        v[v == -1] = 1\n",
    "    else:\n",
    "        v[v != 29] = 0\n",
    "        v[v == 29] = 1\n",
    "    dist = scipy.ndimage.morphology.distance_transform_edt(b)\n",
    "    max_ = np.amax(dist[v == 1])\n",
    "    min_ = np.amin(dist[v == 1])\n",
    "    mid_ = np.median(dist[v == 1])\n",
    "    mean = np.mean(dist[v == 1])\n",
    "    return max_, min_, mid_, mean\n",
    "\n",
    "def dist_topo2gland(volume, bnd, merge_gland):\n",
    "    '''\n",
    "    Compute the distance from topo structures to nearest glandular voxels.\n",
    "    volume and bnd are both binary mask.\n",
    "    '''\n",
    "    import scipy.ndimage\n",
    "    v = volume.copy()\n",
    "    b = bnd.copy()\n",
    "    if (merge_gland):\n",
    "        label_list = [29, 95, 125, 150, 225]\n",
    "        for n in label_list:\n",
    "            v[v == n] = -1\n",
    "        v[v != -1] = 0\n",
    "        v[v == -1] = 1\n",
    "    else:\n",
    "        v[v != 29] = 0\n",
    "        v[v == 29] = 1\n",
    "    v = 1 - v; \n",
    "    dist = scipy.ndimage.morphology.distance_transform_edt(v)\n",
    "    max_ = np.amax(dist[b == 1])\n",
    "    min_ = np.amin(dist[b == 1])\n",
    "    mid_ = np.median(dist[b == 1])\n",
    "    mean = np.mean(dist[b == 1])\n",
    "    return max_, min_, mid_, mean\n",
    "\n",
    "config = [\"dense\", \"hetero\", \"scattered\", \"fatty\"]\n",
    "fileID = [\"p_715565079\", \"p_-884235751\", \"p_1063873867\", \"p_2119279428\"]\n",
    "for mod in range(4):\n",
    "    file_path = \"E:/Data2/VICTRE/synthetic_data/\"+config[mod]+\"_250um/\"+config[mod]+\"_250um_01/\" + fileID[mod] +\".raw\"\n",
    "    topo_path = 'E:/Data2/VICTRE/synthetic_data/'+config[mod]+'_250um/data_'+config[mod]+'_250um_01/dfactor3_padmin_dummysimu_inverse'\n",
    "    volume, shape = FileIO.read_VICTRE_raw(file_path)\n",
    "    bnd = read_topo_chunks(topo_path, shape, 3, 0.0975, \"birth\")\n",
    "    bnd_draw_d1 = resemble_topo_chunks(bnd, shape, 3, 1, True)\n",
    "    bnd_draw_d2 = resemble_topo_chunks(bnd, shape, 3, 2, True)\n",
    "    bnd_draw = bnd_draw_d1 | bnd_draw_d2\n",
    "    print(\"[\" + config[mod] + \"] results:\")\n",
    "    compute_percentage_VICTRE(bnd_draw, volume)\n",
    "    print(\"*************************************************************\")\n",
    "    bnd = read_topo_chunks(topo_path, shape, 3, 0.0, \"birth\")\n",
    "    bnd_draw_d1 = resemble_topo_chunks(bnd, shape, 3, 1, True)\n",
    "    bnd_draw_d2 = resemble_topo_chunks(bnd, shape, 3, 2, True)\n",
    "    bnd_draw = bnd_draw_d1 | bnd_draw_d2\n",
    "    max_g2td1, min_g2td1, mid_g2td1, mean_g2td1 = dist_gland2topo(volume, bnd_draw_d1, True)\n",
    "    max_t2gd1, min_t2gd1, mid_t2gd1, mean_t2gd1 = dist_topo2gland(volume, bnd_draw_d1, True)\n",
    "    max_g2td2, min_g2td2, mid_g2td2, mean_g2td2 = dist_gland2topo(volume, bnd_draw_d2, True)\n",
    "    max_t2gd2, min_t2gd2, mid_t2gd2, mean_t2gd2 = dist_topo2gland(volume, bnd_draw_d2, True)\n",
    "    max_g2td12, min_g2td12, mid_g2td12, mean_g2td12 = dist_gland2topo(volume, bnd_draw, True)\n",
    "    max_t2gd12, min_t2gd12, mid_t2gd12, mean_t2gd12 = dist_topo2gland(volume, bnd_draw, True)\n",
    "\n",
    "    print(\"[Nearest distance from tissue to topo] min 1d 2d 12d: \", min_g2td1, min_g2td2, min_g2td12, \"max 1d 2d 12d: \", max_g2td1, max_g2td2, max_g2td12, \"mid 1d 2d 12d: \", mid_g2td1, mid_g2td2, mid_g2td12, \"mean 1d 2d 12d: \", mean_g2td1, mean_g2td2, mean_g2td12)\n",
    "    print(\"[Nearest distance from topo to tissue] min 1d 2d 12d: \", min_t2gd1, min_t2gd2, min_t2gd12, \"max 1d 2d 12d: \", max_t2gd1, max_t2gd2, max_t2gd12, \"mid 1d 2d 12d: \", mid_t2gd1, mid_t2gd2, mid_t2gd12, \"mean 1d 2d 12d: \", mean_t2gd1, mean_t2gd2, mean_t2gd12)\n",
    "    print(\"=============================================================\")\n",
    "    print(\"=============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index = 200\n",
    "img_copy = MRI_[index,:,:].copy()\n",
    "img1 = np.transpose(np.flip(np.flip(img_copy),axis=0))\n",
    "#img1[img1 == -0.13071264] = 0.0\n",
    "#img1 = np.amax(img1) - img1\n",
    "#print(np.amax(img1), np.amin(img1))\n",
    "#img1[196: 198, 150: 200] = 0\n",
    "#print(img1[197, 151])\n",
    "\n",
    "bg = np.zeros(img1.shape)\n",
    "subbg = np.zeros(img1.shape)\n",
    "fg = np.zeros(img1.shape)\n",
    "bg[img1 == 0.0] = 1\n",
    "fg[img1 > -0.180] = 1\n",
    "fg[bg == 1] = 0\n",
    "subbg[fg == 0] = 1\n",
    "subbg[bg == 1] = 0\n",
    "\n",
    "#fg[subbg == 1] = 0.18\n",
    "\n",
    "uniq_arr = img1[fg==1]\n",
    "norm_arr = (uniq_arr-np.amin(uniq_arr))/(np.amax(uniq_arr)-np.amin(uniq_arr))\n",
    "norm_arr[norm_arr < 0.5] += 0.6\n",
    "fg[fg == 1] = norm_arr\n",
    "fg[subbg == 1] = 0.09\n",
    "#norm_arr = norm_arr - np.amin(norm_arr)\n",
    "#fg[fg == 1] = norm_arr\n",
    "\n",
    "# print(np.amax(img1), np.amin(img1))\n",
    "# img1[img1 == 0.0] = np.amin(img1) - 0.02\n",
    "# for i in range(img1.shape[0]):\n",
    "#     for j in range(img1.shape[1]):\n",
    "#         if (img1[i][j] > - 0.18):\n",
    "#             img1[i][j] = img1[i][j] * \n",
    "# #img1[img1 > -0.180] =100\n",
    "# #img1[img1 < -0.180] = np.amin(img1)-0.1\n",
    "# #img1[img1 == 0.0] = np.amin(img1)-0.02\n",
    "# #img1[img1 == 0.0] = np.amin(img1) - 0.03\n",
    "\n",
    "# print(np.amax(img1), np.amin(img1))\n",
    "#viewer.imshow_(fg)\n",
    "\n",
    "# for index in range(0, MRI_.shape[0], 5):\n",
    "#     print(index)\n",
    "#     img_copy = MRI_[index,:,:].copy()\n",
    "#     img1 = np.transpose(np.flip(np.flip(img_copy),axis=0))\n",
    "#     img1 = np.amax(img1) - img1\n",
    "#     viewer.imshow_(img1)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(fg, cmap='gray')\n",
    "plt.axis('off')  # Remove axes\n",
    "plt.savefig('C:/Users/Fan Wang/Downloads/hetero01_200_MRI.png', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_overlap_view(data, data2, vmin_ = 0.0, vmax_ = 1.0, opacity=1.0):\n",
    "    mlab.figure(bgcolor=(0.1, 0.1, 0.1), size=(600, 600))\n",
    "    src = mlab.pipeline.scalar_field(data)\n",
    "    src.spacing = [1, 1, 1]\n",
    "    src.update_image_data = True\n",
    "\n",
    "    src2 = mlab.pipeline.scalar_field(data2)\n",
    "    src2.spacing = [1, 1, 1]\n",
    "    src2.update_image_data = True\n",
    "\n",
    "\n",
    "    voi1 = mlab.pipeline.extract_grid(src)\n",
    "    voi2 = mlab.pipeline.extract_grid(src2)\n",
    "\n",
    "    thr = mlab.pipeline.threshold(voi2, low=0.28)\n",
    "    cut_plane = mlab.pipeline.scalar_cut_plane(thr,\n",
    "                                    plane_orientation='x_axes',\n",
    "                                    colormap='black-white',\n",
    "                                    vmin=0,\n",
    "                                    vmax=1)\n",
    "    cut_plane.implicit_plane.origin = (200, 0, 0)\n",
    "    cut_plane.implicit_plane.widget.enabled = False\n",
    "\n",
    "    outer = mlab.pipeline.iso_surface(voi1, colormap='jet', vmin=vmin_, vmax=vmax_, opacity=opacity)\n",
    "    mlab.show()\n",
    "    \n",
    "def slice_overlap_view_wBoundary(data, data2, data3, vmin_ = 0.0, vmax_ = 1.0, opacity=1.0):\n",
    "    mlab.figure(bgcolor=(0.1, 0.1, 0.1), size=(600, 600))\n",
    "    src = mlab.pipeline.scalar_field(data)\n",
    "    src.spacing = [1, 1, 1]\n",
    "    src.update_image_data = True\n",
    "\n",
    "    src2 = mlab.pipeline.scalar_field(data2)\n",
    "    src2.spacing = [1, 1, 1]\n",
    "    src2.update_image_data = True\n",
    "    \n",
    "    src3 = mlab.pipeline.scalar_field(data3)\n",
    "    src3.spacing = [1, 1, 1]\n",
    "    src3.update_image_data = True\n",
    "\n",
    "    voi1 = mlab.pipeline.extract_grid(src)\n",
    "    voi2 = mlab.pipeline.extract_grid(src2)\n",
    "    voi3 = mlab.pipeline.extract_grid(src3)\n",
    "\n",
    "    thr = mlab.pipeline.threshold(voi2, low=0.28)\n",
    "    cut_plane = mlab.pipeline.scalar_cut_plane(thr,\n",
    "                                    plane_orientation='x_axes',\n",
    "                                    colormap='black-white',\n",
    "                                    vmin=0,\n",
    "                                    vmax=1)\n",
    "    cut_plane.implicit_plane.origin = (200, 0, 0)\n",
    "    cut_plane.implicit_plane.widget.enabled = False\n",
    "\n",
    "    outer = mlab.pipeline.iso_surface(voi1, colormap='jet', vmin=vmin_, vmax=vmax_, opacity=opacity)\n",
    "    outer = mlab.pipeline.iso_surface(voi3, colormap='hot', vmin=vmin_, vmax=vmax_, opacity=opacity)\n",
    "    mlab.show()\n",
    "\n",
    "left_ = 190\n",
    "right_ = 210\n",
    "\n",
    "v = volume.copy()\n",
    "label_list = [29, 88, 95, 125, 150]\n",
    "for n in label_list:\n",
    "    v[v == n] = -1\n",
    "v[v != -1] = 0\n",
    "v[v == -1] = 1\n",
    "    \n",
    "b = bnd_draw.copy()\n",
    "b[:left_,:,:] = 0\n",
    "b[right_:,:,:] = 0\n",
    "\n",
    "bound = volume.copy()\n",
    "bound[bound!=2] = 0\n",
    "bound[bound==2] = 1\n",
    "bound[:left_,:,:] = 0\n",
    "bound[right_:,:,:] = 0\n",
    "\n",
    "#slice_overlap_view(b, v)\n",
    "slice_overlap_view_wBoundary(b, v, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_gland2topo(volume, bnd, merge_gland):\n",
    "    '''\n",
    "    Compute the distance from glandular voxels to nearest topo structures.\n",
    "    volume and bnd are both binary mask.\n",
    "    '''\n",
    "    import scipy.ndimage\n",
    "    v = volume.copy()\n",
    "    b = 1 - bnd.copy()\n",
    "    if (merge_gland):\n",
    "        label_list = [29, 95, 125, 150, 225]      # Correspond to Glandular, TDLU, Duct, Artery, Vein resp.\n",
    "        for n in label_list:\n",
    "            v[v == n] = -1\n",
    "        v[v != -1] = 0\n",
    "        v[v == -1] = 1\n",
    "    else:\n",
    "        v[v != 29] = 0\n",
    "        v[v == 29] = 1\n",
    "    dist = scipy.ndimage.morphology.distance_transform_edt(b)\n",
    "    max_ = np.amax(dist[v == 1])\n",
    "    min_ = np.amin(dist[v == 1])\n",
    "    mid_ = np.median(dist[v == 1])\n",
    "    mean = np.mean(dist[v == 1])\n",
    "    return max_, min_, mid_, mean\n",
    "\n",
    "def dist_topo2gland(volume, bnd, merge_gland):\n",
    "    '''\n",
    "    Compute the distance from topo structures to nearest glandular voxels.\n",
    "    volume and bnd are both binary mask.\n",
    "    '''\n",
    "    import scipy.ndimage\n",
    "    v = volume.copy()\n",
    "    b = bnd.copy()\n",
    "    if (merge_gland):\n",
    "        label_list = [29, 95, 125, 150, 225]\n",
    "        for n in label_list:\n",
    "            v[v == n] = -1\n",
    "        v[v != -1] = 0\n",
    "        v[v == -1] = 1\n",
    "    else:\n",
    "        v[v != 29] = 0\n",
    "        v[v == 29] = 1\n",
    "    v = 1 - v; \n",
    "    dist = scipy.ndimage.morphology.distance_transform_edt(v)\n",
    "    max_ = np.amax(dist[b == 1])\n",
    "    min_ = np.amin(dist[b == 1])\n",
    "    mid_ = np.median(dist[b == 1])\n",
    "    mean = np.mean(dist[b == 1])\n",
    "    return max_, min_, mid_, mean\n",
    "\n",
    "def compute_percentage_VICTRE(bnd_draw, volume):\n",
    "    '''\n",
    "    @bnd_draw: binary mask indicating the topo structures\n",
    "    @volume: integer mask indicating different breast parts starting from 0\n",
    "    @rec_tp_percentage: #overlapping voxels / #total topo voxels\n",
    "    @rec_rc_percentage: #overlapping voxels / #breast part voxels\n",
    "    '''\n",
    "    volume_backup = volume.copy()\n",
    "    total = np.sum(bnd_draw==1)\n",
    "    volume[bnd_draw != 1] = -1\n",
    "    rec_tp_percentage = [0.0] * len(np.unique(volume))\n",
    "    rec_rc_percentage = [0.0] * len(np.unique(volume))\n",
    "    for i in range(len(np.unique(volume))):\n",
    "        if np.unique(volume)[i] == -1:\n",
    "            continue\n",
    "        rec_tp_percentage[i] = np.sum(volume == np.unique(volume)[i]) / total\n",
    "        rec_rc_percentage[i] = np.sum(volume == np.unique(volume)[i]) / np.sum(volume_backup == np.unique(volume)[i])\n",
    "    print(\"#overlapping voxels / #total topo voxels\")\n",
    "    for i in range(len(np.unique(volume))):\n",
    "        print(np.unique(volume)[i], rec_tp_percentage[i])\n",
    "    print(\"#overlapping voxels / #breast part voxels\")\n",
    "    for i in range(len(np.unique(volume))):\n",
    "        print(np.unique(volume)[i], rec_rc_percentage[i])\n",
    "        \n",
    "    #===== Merge glandular, ligament, TDLU, duct, artery into one\n",
    "    target_labels = [29, 88, 95, 125, 150]\n",
    "    rec_merge_tp_avg = 0.0\n",
    "    rec_merge_rc_overlap = 0\n",
    "    rec_merge_rc_parttot = 0\n",
    "    for i in range(len(np.unique(volume))):\n",
    "        if np.unique(volume)[i] in target_labels:\n",
    "            rec_merge_tp_avg = rec_merge_tp_avg + rec_tp_percentage[i]\n",
    "            rec_merge_rc_overlap = rec_merge_rc_overlap + np.sum(volume == np.unique(volume)[i])\n",
    "            rec_merge_rc_parttot = rec_merge_rc_parttot + np.sum(volume_backup == np.unique(volume)[i])\n",
    "    print(\"total ovp/total vox: \", rec_merge_tp_avg)\n",
    "    print(\"total ovp/total breast vox: \", rec_merge_rc_overlap/rec_merge_rc_parttot)\n",
    "    volume = volume_backup\n",
    "\n",
    "volume_backup = volume.copy()\n",
    "bnd_draw_copy = bnd_draw.copy()\n",
    "#bnd_draw_copy[volume_backup == 0] = 0\n",
    "compute_percentage_VICTRE(bnd_draw_copy, volume_backup)\n",
    "\n",
    "# # ===== Compute distance from glandular to nearest topo\n",
    "# max_g2td1, min_g2td1, mid_g2td1, mean_g2td1 = dist_gland2topo(volume, bnd_draw_d1, True)\n",
    "# max_t2gd1, min_t2gd1, mid_t2gd1, mean_t2gd1 = dist_topo2gland(volume, bnd_draw_d1, True)\n",
    "\n",
    "# max_g2td2, min_g2td2, mid_g2td2, mean_g2td2 = dist_gland2topo(volume, bnd_draw_d2, True)\n",
    "# max_t2gd2, min_t2gd2, mid_t2gd2, mean_t2gd2 = dist_topo2gland(volume, bnd_draw_d2, True)\n",
    "\n",
    "# max_g2td12, min_g2td12, mid_g2td12, mean_g2td12 = dist_gland2topo(volume, bnd_draw, True)\n",
    "# max_t2gd12, min_t2gd12, mid_t2gd12, mean_t2gd12 = dist_topo2gland(volume, bnd_draw, True)\n",
    "\n",
    "# #print(min_g2td1, max_g2td1, mid_g2td1, mean_g2td1)\n",
    "\n",
    "# print(min_g2td1, min_g2td2, min_g2td12, max_g2td1, max_g2td2, max_g2td12, mid_g2td1, mid_g2td2, mid_g2td12, mean_g2td1, mean_g2td2, mean_g2td12)\n",
    "# print(min_t2gd1, min_t2gd2, min_t2gd12, max_t2gd1, max_t2gd2, max_t2gd12, mid_t2gd1, mid_t2gd2, mid_t2gd12, mean_t2gd1, mean_t2gd2, mean_t2gd12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mayavi import mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as axes3d\n",
    "\n",
    "def tmp_viewer_(data, smooth=False, opacity=1.0):\n",
    "        mlab.figure(bgcolor=(0.2, 0.2, 0.2), size=(600, 600))\n",
    "        src = mlab.pipeline.scalar_field(data)\n",
    "        src.spacing = [1, 1, 1]\n",
    "        src.update_image_data = True\n",
    "        if smooth:\n",
    "            blur = mlab.pipeline.user_defined(src, filter='ImageGaussianSmooth')\n",
    "            voi2 = mlab.pipeline.extract_grid(blur)\n",
    "        else:\n",
    "            voi2 = mlab.pipeline.extract_grid(src)\n",
    "        outer = mlab.pipeline.iso_surface(voi2, colormap='jet', vmin=0.0, vmax=2.0, opacity=opacity)\n",
    "        mlab.show()\n",
    "\n",
    "def tmp2_viewer_(data, tps):\n",
    "    assert(data.shape == tps.shape)\n",
    "    mlab.figure(bgcolor=(0.2, 0.2, 0.2), size=(600, 600))\n",
    "    src_data = mlab.pipeline.scalar_field(data)\n",
    "    src_data.spacing = [1, 1, 1]\n",
    "    src_data.update_image_data = True\n",
    "\n",
    "    src_tps = mlab.pipeline.scalar_field(tps)\n",
    "    src_tps.spacing = [1, 1, 1]\n",
    "    src_tps.update_image_data = True\n",
    "\n",
    "    voi1 = mlab.pipeline.extract_grid(src_data)\n",
    "    #voi1.trait_set(y_min=39)\n",
    "    outer1 = mlab.pipeline.iso_surface(voi1, colormap='hot', vmin=0.0, vmax=1.5, opacity=0.5)\n",
    "\n",
    "    voi2 = mlab.pipeline.extract_grid(src_tps)\n",
    "    #voi2.trait_set(y_min=39)\n",
    "    outer2 = mlab.pipeline.iso_surface(voi2, colormap='jet', vmin=0.0, vmax=np.amax(tps)+2, opacity=0.6)\n",
    "\n",
    "#         thr = mlab.pipeline.threshold(src_data, low=40)\n",
    "#         cut_plane = mlab.pipeline.scalar_cut_plane(thr,\n",
    "#                                         plane_orientation='y_axes',\n",
    "#                                         colormap='black-white',\n",
    "#                                         vmin=200,\n",
    "#                                         vmax=800)\n",
    "#         cut_plane.implicit_plane.widget.enabled = False\n",
    "    mlab.show()\n",
    "\n",
    "bnd_draw_ = bnd_draw.copy()\n",
    "bnd_draw_[volume == 2] = 0\n",
    "#bnd_draw_[volume == 40] = 0\n",
    "bnd_draw_[volume == 0] = 0\n",
    "##bnd_draw_[volume == 1] = 0\n",
    "\n",
    "volume_ = volume.copy()\n",
    "volume_[volume_ != 29] = 0\n",
    "volume_[volume_ == 29] = 1\n",
    "\n",
    "tmp2_viewer_(volume_, bnd_draw_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def slice_overlap_view2D_loop(vol, bnd):\n",
    "    for slice_ind in range(12, bnd.shape[0], 4):\n",
    "        print(slice_ind)\n",
    "        slice = slice_overlap_view2D(vol, bnd, slice_ind)\n",
    "        viewer.imshow_(slice)\n",
    "        \n",
    "def slice_overlap_view2D(vol, bnd, slice_ind):\n",
    "    kernel = np.ones((2,2), 'uint8')\n",
    "    bnd_slice = np.squeeze(bnd[slice_ind, :, :])\n",
    "    vol_slice = np.squeeze(vol[slice_ind, :, :])\n",
    "    bnd_slice = bnd_slice.astype('float32')\n",
    "    vol_slice = vol_slice.astype('float32')\n",
    "    bnd_slice1 = cv2.dilate(bnd_slice, kernel, iterations=5)\n",
    "    bnd_slice2 = cv2.dilate(bnd_slice, kernel, iterations=3)\n",
    "    vol_slice = cv2.dilate(vol_slice, kernel, iterations=1)\n",
    "    \n",
    "    h = vol_slice.shape[0]\n",
    "    w = vol_slice.shape[1]\n",
    "    \n",
    "    wanted = np.zeros(vol_slice.shape, np.int32)\n",
    "    for i in range(h):\n",
    "            for j in range(w):\n",
    "                if bnd_slice1[i,j] != bnd_slice2[i,j] and vol_slice[i,j] == 1:\n",
    "                    wanted[i,j] = 1\n",
    "    bnd_slice3 = np.zeros(vol_slice.shape, np.float32)\n",
    "    for i in range(h):\n",
    "            for j in range(w):\n",
    "                if wanted[i,j] == 1 or bnd_slice2[i,j] != 0:\n",
    "                    bnd_slice3[i,j] = 1\n",
    "                    \n",
    "    vol_rgb = np.zeros(vol_slice.shape, np.int32)\n",
    "    bnd_rgb = np.zeros(bnd_slice3.shape, np.int32)\n",
    "    vol_rgb = cv2.cvtColor(vol_slice, cv2.COLOR_GRAY2BGR)\n",
    "    bnd_rgb = cv2.cvtColor(bnd_slice3, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    for i in range(h):\n",
    "            for j in range(w):\n",
    "                if bnd_slice3[i,j] == 1 and vol_slice[i,j] == 1:\n",
    "                    vol_rgb[i,j,:] = (0.0, 0.6, 1.0)\n",
    "                if bnd_slice3[i,j] == 1 and vol_slice[i,j] == 0:\n",
    "                    vol_rgb[i,j,:] = (0.7, 0.0, 0.0)\n",
    "    return vol_rgb\n",
    "\n",
    "v = volume.copy()\n",
    "label_list = [29, 88, 95, 125, 150]\n",
    "for n in label_list:\n",
    "    v[v == n] = -1\n",
    "v[v != -1] = 0\n",
    "v[v == -1] = 1\n",
    "    \n",
    "b = bnd_draw.copy()\n",
    "#b[b == 0] = 0\n",
    "\n",
    "slice = slice_overlap_view2D(v, b, 200)\n",
    "slice = np.rot90(slice)\n",
    "plt.figure(figsize = (15, 15))\n",
    "viewer.imshow_(slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run viewer.ipynb\n",
    "%run Medical_IO.ipynb\n",
    "%run Medical_Utility.ipynb\n",
    "%run Cyclekernel.ipynb\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from skimage import io as skio\n",
    "\n",
    "def nz2_nearest_z(t):\n",
    "    return scipy.ndimage.morphology.distance_transform_edt(t)\n",
    "\n",
    "# file_path_img = \"E:/Data2/chunk_exp/cremi_03835.png\"\n",
    "# img = cv2.imread(file_path_img, cv2.IMREAD_GRAYSCALE)\n",
    "# dist_trfm = nz2_nearest_z(img)\n",
    "\n",
    "# shape, chunks = chop_into_chunks(dist_trfm, 2, False, True, \"min\")\n",
    "\n",
    "# for cc in chunks:\n",
    "#     viewer.imshow_(cc)\n",
    "\n",
    "# vol_ = resemble_chunks(chunks, shape, 2, True)\n",
    "# save_chunks(chunks, shape, 2, 'E:/Data2/chunk_exp')\n",
    "# bnd = read_topo_chunks('E:/Data2/chunk_exp', [64, 64], 2, 0.0)\n",
    "# bnd_draw = resemble_topo_chunks(bnd, [64, 64], 2, 1, True)\n",
    "\n",
    "# FileIO_MEDICAL.write_dat(dist_trfm, \"E:/Data2/chunk_exp/cremi_03835.png.dat\")\n",
    "# dat = FileIO_MEDICAL.read_dat(\"E:/Data2/cremi_03061.png.dat\")\n",
    "\n",
    "# pers = FileIO_MEDICAL.read_pers_txt('E:/Data2/chunk_exp/cremi_03835.png.dat.pers.txt')\n",
    "# bnd  = FileIO_MEDICAL.read_bnd_red_unifieddim('E:/Data2/chunk_exp/cremi_03835.png.dat.bnd')\n",
    "# bnd_filt = Compute_CycleKernel.filter_bnd_or_red(bnd, pers, 0.0)\n",
    "# bnd_draw = Utility_MEDICAL.draw_on_plane(bnd_filt, [64, 64], 1, np.arange(0,len(bnd_filt[1])))\n",
    "# viewer.imshow_(bnd_draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medcam import medcam\n",
    "\n",
    "vgg16 = medcam.inject(vgg16, output_dir='E:/Data2/gradcam_attention_maps', backend='ggcam', layer='layer5', label='best', save_maps=True)\n",
    "vgg16.eval()\n",
    "\n",
    "def load_image(image_path):\n",
    "    raw_image = cv2.imread(image_path)\n",
    "    raw_image = cv2.resize(raw_image, (224,) * 2)\n",
    "    image = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )(raw_image[..., ::-1].copy())\n",
    "     \n",
    "    \n",
    "#     image = transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(224),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "#                              std  = [ 0.229, 0.224, 0.225 ]),\n",
    "#         ])(raw_image[..., ::-1].copy())\n",
    "    \n",
    "    #image = image.to(device)\n",
    "    image = image.cuda()\n",
    "    return image.unsqueeze(0)\n",
    "    #return image\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#model = models.resnet50(pretrained=False)\n",
    "#model = VGG16(2)\n",
    "#vgg16.to(device=device)\n",
    "#vgg16.eval()\n",
    "\n",
    "path = 'E:/Data2/Dog_Cat/train_subset/dog/dogcat_03617.jpg'\n",
    "img = load_image(path)\n",
    "out = vgg16(img)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medcam import medcam\n",
    "\n",
    "vgg16 = medcam.inject(vgg16, output_dir='E:/Data2/gradcam_attention_maps', backend='ggcam', layer='layer5', label='best', save_maps=True)\n",
    "vgg16.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    raw_image = cv2.imread(image_path)\n",
    "#     raw_image = cv2.resize(raw_image, (224,) * 2)\n",
    "#     image = transforms.Compose(\n",
    "#         [\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#         ]\n",
    "#     )(raw_image[..., ::-1].copy())\n",
    "    \n",
    "    \n",
    "    \n",
    "    image = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "                             std  = [ 0.229, 0.224, 0.225 ]),\n",
    "        ])(raw_image[..., ::-1].copy())\n",
    "    \n",
    "    #image = image.to(device)\n",
    "    #image = image.cuda()\n",
    "    #return image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#model = models.resnet50(pretrained=False)\n",
    "#model = VGG16(2)\n",
    "#vgg16.to(device=device)\n",
    "#vgg16.eval()\n",
    "\n",
    "path = 'E:/Data2/Dog_Cat/train/cat.4.jpg'\n",
    "img = load_image(path)\n",
    "_ = vgg16(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.vgg16(pretrained=False)\n",
    "#model = models.densenet161(pretrained=False)\n",
    "#print(model.features.denseblock4.denselayer24)\n",
    "print(model.features[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy, scipy , pylab , wave, scipy.fftpack\n",
    "%run FileIO.ipynb\n",
    "\n",
    "def fftind(dim, size):\n",
    "    '''\n",
    "    @size: size of the data, has to be square\n",
    "    @dim: 2 or 3\n",
    "    '''\n",
    "    if dim == 2:\n",
    "        k_ind = numpy.mgrid[:size, :size] - int( (size + 1)/2 )\n",
    "    else:\n",
    "        k_ind = numpy.mgrid[:size, :size, :size] - int( (size + 1)/2 ) \n",
    "    k_ind = scipy.fftpack.fftshift(k_ind)\n",
    "    return( k_ind )\n",
    "\n",
    "def gaussian_random_field(dim, alpha, size, round_decimal, flag_normalize = True):\n",
    "        \n",
    "    # Defines momentum indices\n",
    "    k_idx = fftind(dim, size)\n",
    "\n",
    "    # Defines the amplitude as a power law 1/|k|^(alpha/2)\n",
    "    if dim == 2:\n",
    "        amplitude = numpy.power( k_idx[0]**2 + k_idx[1]**2 + 1e-10, -alpha/4.0 )\n",
    "        amplitude[0,0] = 0\n",
    "    else:\n",
    "        amplitude = numpy.power( k_idx[0]**2 + k_idx[1]**2 + k_idx[2]**2 + 1e-10, -alpha/4.0 )\n",
    "        amplitude[0,0,0] = 0\n",
    "    \n",
    "    # Draws a complex gaussian random noise with normal\n",
    "    # (circular) distribution\n",
    "    if dim == 2:\n",
    "        noise = numpy.random.normal(size = (size, size)) \\\n",
    "            + 1j * numpy.random.normal(size = (size, size))\n",
    "        gfield = numpy.fft.ifft2(noise * amplitude).real\n",
    "    else:\n",
    "        noise = numpy.random.normal(size = (size, size, size)) \\\n",
    "            + 1j * numpy.random.normal(size = (size, size, size))\n",
    "        gfield = numpy.fft.ifftn(noise * amplitude).real\n",
    "    \n",
    "    # Sets the standard deviation to one\n",
    "    if flag_normalize:\n",
    "        gfield = gfield - numpy.mean(gfield)\n",
    "        gfield = gfield/numpy.std(gfield)\n",
    "        \n",
    "    return np.matrix.round(gfield.astype(numpy.float32), round_decimal)\n",
    "\n",
    "def vis_grf_(dim, alpha, data):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    %run viewer_3D.ipynb\n",
    "    \n",
    "    if dim == 2:\n",
    "        my_dpi = 128\n",
    "        plt.figure(\n",
    "            figsize = (data.shape[1]/my_dpi, data.shape[0]/my_dpi),\n",
    "            dpi = my_dpi)\n",
    "        plt.axis('off')\n",
    "        plt.title('alpha='+str(alpha))\n",
    "        plt.imshow(data, interpolation='none')\n",
    "    else:\n",
    "        viewer_3D.tmp_viewer(data, numpy.amin(data), numpy.amax(data), opacity=0.5)\n",
    "        \n",
    "def uniform_random(path, dim, size):\n",
    "    from random import randrange\n",
    "    out = open(path, \"a+b\")\n",
    "    \n",
    "    if dim == 2:\n",
    "        buf = np.asarray([0.0] * size)\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                buf[j] = randrange(256)\n",
    "            out.write(struct.pack(str(size)+'f', *(buf)))\n",
    "            if i % 32 == 0:\n",
    "                print(\"Current progress: \", i * 1.0 / size, \"%\")\n",
    "    else:\n",
    "        buf = np.asarray([[0.0] * size] * size)\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                for k in range(size):\n",
    "                    buf[j][k] = randrange(256)\n",
    "            out.write(struct.pack(str(size*size)+'f', *(buf.flatten())))    \n",
    "            if i % 32 == 0:\n",
    "                print(\"Current progress: \", i * 1.0 / size, \"%\")\n",
    "    out.close()\n",
    "    \n",
    "def binarized_sample(path, dim, size):\n",
    "    from random import randrange\n",
    "    out = open(path, \"a+b\")\n",
    "    \n",
    "    if dim == 2:\n",
    "        buf = np.asarray([0.0] * size)\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                buf[j] = randrange(2)\n",
    "            out.write(struct.pack(str(size)+'f', *(buf)))\n",
    "#             if i % 32 == 0:\n",
    "#                 print(\"Current progress: \", i * 1.0 / size, \"%\")\n",
    "    else:\n",
    "        buf = np.asarray([[0.0] * size] * size)\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                for k in range(size):\n",
    "                    buf[j][k] = randrange(2)\n",
    "            out.write(struct.pack(str(size*size)+'f', *(buf.flatten())))\n",
    "#             if i % 32 == 0:\n",
    "#                 print(\"Current progress: \", i * 1.0 / size, \"%\")\n",
    "    out.close()\n",
    "\n",
    "# Generate GRF data\n",
    "size_to_generate = 8192\n",
    "num_to_generate = 10\n",
    "for i in range(0, num_to_generate):\n",
    "    zeropadded_index = str(i)\n",
    "    for j in range(len(str(num_to_generate)) - len(str(i)) - 1):\n",
    "        zeropadded_index = \"0\" + zeropadded_index\n",
    "    #binarized_sample(\"E:/Data2/SoCG2022/Stitch_jocg/2D/4096/\" + \"2D_4096_\" + str(i) + \".dat\", 2, 4096)\n",
    "    example = gaussian_random_field(2, alpha = 7.5, size = size_to_generate, round_decimal = 3) * 10000\n",
    "    example = example.astype(np.int32)\n",
    "    FileIO.write_binary(\"E:/Data2/SoCG2022/GaussRandomField/2Dint/Alpha7.5/8192/2D_\" + str(size_to_generate) + \"_\" + zeropadded_index + \".dat\", example.flatten(), [size_to_generate, size_to_generate], 'i')\n",
    "\n",
    "  \n",
    "# size_ = 64\n",
    "# example = gaussian_random_field(2, alpha = 1.0, size = size_, round_decimal = 2)\n",
    "# example = example[0:16,:]\n",
    "\n",
    "# FileIO.write_binary(\"E:/Data2/SoCG2022/GaussRandomField/2D/2D_16x64_\" + str(0) + \".dat\", example.flatten(), [16, 64], 'f')\n",
    "\n",
    "#uniform_random(\"E:/Data2/SoCG2022/UniformRandom/3D/3D_256_0.dat\", 3, 256)\n",
    "# example = gaussian_random_field(2, alpha = 1.0, size = 256, round_decimal = 2)\n",
    "# vis_grf_(2, 2.0, example)\n",
    "#FileIO.write_binary(\"E:/Data2/SoCG2022/GaussRandomField/3D/3D_256_9.dat\", example.flatten(), [256, 256, 256], 'f')\n",
    "\n",
    "\n",
    "# dim = 3\n",
    "# instance_num = 1\n",
    "# #shape_candidate = [32, 128, 512, 1024, 2048, 4096, 8192]\n",
    "# shape_candidate = [512]\n",
    "# for shape in shape_candidate:\n",
    "#     for i in range(instance_num):\n",
    "#         output_path = \"E:/Data2/SoCG2022/GaussRandomField/\" + str(dim) + \"D_\" + str(shape) + \"_\" + str(i) + \".dat\"\n",
    "#         example = gaussian_random_field(dim, alpha = 3.0, size = shape, round_decimal = 2)\n",
    "#         if dim == 2:\n",
    "#             FileIO.write_binary(output_path, example.flatten(), [shape, shape], 'f')\n",
    "#         else:\n",
    "#             FileIO.write_binary(output_path, example.flatten(), [shape, shape, shape], 'f')\n",
    "        \n",
    "#example = gaussian_random_field(2, alpha = 3.0, size = 16384)\n",
    "#FileIO.write_binary(\"E:/Data2/SoCG2022/GaussRandomField/t.dat\", example.flatten(), [128, 128], 'f')\n",
    "#example = FileIO.read_binary(\"E:/Data2/SoCG2022/GaussRandomField/t.dat\", [128, 128], 'f')\n",
    "#vis_grf_(2, 3.0, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# norm = (example - np.min(example)) / (np.max(example) - np.min(example)) * len(np.unique(example))\n",
    "# norm = np.floor(norm)\n",
    "\n",
    "# print(len(np.unique(norm)))\n",
    "# print(np.amin(norm), np.amax(norm))\n",
    "\n",
    "# FileIO.write_binary(\"E:/Data2/SoCG2022/Repeated_Gaussian/2D_1024_11.dat\", norm.flatten(), [1024, 1024], 'f')\n",
    "# vis_grf_(2, 3.0, norm)\n",
    "\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "example = FileIO.read_binary(\"E:/Data2/SoCG2022/Repeated_Gaussian/coin/coin.dat\", [256, 256], 'f')\n",
    "cv2.imwrite(\"E:/Data2/SoCG2022/Repeated_Gaussian/coin/Blur_\" + str(0) + \".png\", example)\n",
    "for i in range(1, 20):\n",
    "    example = gaussian_filter(example, sigma=0.6)\n",
    "    cv2.imwrite(\"E:/Data2/SoCG2022/Repeated_Gaussian/coin/Blur_\" + str(i) + \".png\", example)\n",
    "\n",
    "#vis_grf_(2, 3.0, example)\n",
    "# for i in range(50):\n",
    "#     example = gaussian_filter(example, sigma=1.5)\n",
    "#     vis_grf_(2, 3.0, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "%run viewer.ipynb\n",
    "%run FileIO.ipynb\n",
    "\n",
    "path_jpg = \"E:/Data2/SoCG2022/CMB_jpg\"\n",
    "path_dat = \"E:/Data2/SoCG2022/CMB\"\n",
    "shape = [\"150_75\", \"300_150\", \"600_300\", \"900_450\", \"1500_750\", \"3000_1500\", \"6400_3200\"]\n",
    "\n",
    "for i in range(len(shape)):\n",
    "    pathIn = path_jpg + \"/planck13_\" + shape[i] + \".jpg\"\n",
    "    pathOut = path_dat + \"/planck13_\" + shape[i] + \".dat\"\n",
    "    img = cv2.imread(pathIn, 0)\n",
    "    FileIO.write_binary(pathOut, img.flatten(), img.shape, 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "%run viewer.ipynb\n",
    "%run FileIO.ipynb\n",
    "\n",
    "# path = \"E:/Data2/SoCG2022/Repeated_Gaussian/lena.png\"\n",
    "# pathOut = \"E:/Data2/SoCG2022/Repeated_Gaussian/lena.dat\"\n",
    "# img = cv2.imread(path, 0)\n",
    "# img = img.astype(np.float32)\n",
    "# FileIO.write_binary(pathOut, img.flatten(), img.shape, 'f')\n",
    "\n",
    "img = FileIO.read_binary(\"E:/Data2/SoCG2022/UniformRandom/3D/\", [256, 256], 'f')\n",
    "img = img.flatten()\n",
    "print(img[11647])\n",
    "\n",
    "# print(img.shape)\n",
    "# print(img[0][0].dtype)\n",
    "# viewer.imshow_(img)\n",
    "# print(img[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
